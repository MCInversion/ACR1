---
title: "WindACR1"
author: "Martin Cavarga"
date: "5/20/2019"
output:
  html_document:
    toc: true
    theme: united
---

```{r, echo=F}
pkgTest <- function(x) {
  if (!require(x,character.only = TRUE)) {
    install.packages(x,dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}
```

# Methods of Time Series Analysis Applied to Weather Measurements on a Wind Farm 

## Introduction

The chosen source of data is a wind farm located in the North Sea (with coords: 54.01433°NL, 6.587667°EL) denoted as LES (1y 2015-01-01 2015-12-31 cfsr) which comes from website http://www.vortexfdc.com/, specifically designed to provide its
  users with weather data for potential wind farm sites. This specific dataset has been obtained (for free) from year 2015, and contains a variety of measurements, ranging from wind velocity magnitude, through the wind direction angle, temperature up to dimensionless characteristics like  the Richardson number "Ri" which corresponds to the ratio of buoyancy over shear flow.
In the first 7 parts of this project, I focus on the analysis of a single variable, namely temperature T [°C]. Since the dataset consist of over 47 000 observations of 11 different variables, and thus might be difficult to visualize, I've chosen to include only the average temperature measurements for each day.
 
 ---------------------------------------------------------------------------------------

## 1. Visualization and Basic Stats

We begin by extracting the data from a downloaded file

```{r}
vortex_dat <- read.table("vortex_TS.txt", header = T, skip = 3, 
                          colClasses = "character")
```

The dataset contains 47808 values, making it too large for our methods. Thus we will choose daily values, namely the average daily temperature. To get the daily data we need to `aggregate` values with the same date:

```{r}
suppressMessages(pkgTest("dplyr"))
names(vortex_dat)[8] <- c("temperature")

vortex_dat <- tidyr::separate(data = vortex_dat, col = YYYYMMDD, 
                               into = c("year","month","day"), sep=c(4,6))
vortex_dat <- tidyr::separate(data = vortex_dat, col = HHMM, 
                               into = c("hour","minute"), sep=2)

vortex_dat <- tidyr::unite(vortex_dat, col=time, year, month, day, hour, minute, sep="-", remove=T)
vortex_dat$time <- as.Date(vortex_dat$time)

temp <- aggregate(as.numeric(vortex_dat$temperature), by=list(vortex_dat$time), mean)
names(temp) <- c("day", "T")

head(temp)

```

Now we can visualize the data as followed:

```{r basicDataPlot1, fig.width=9, fig.height=6}
suppressMessages(pkgTest("rmarkdown"))
suppressMessages(pkgTest("knitr"))

temp$time <- 1:nrow(temp)

par(mfrow=c(1,1))
plot(x=temp$time, y=temp$T, type="p",xlab="day", ylab="T[°C]",main="Daily temperature",axes=F)
lines(x=temp$time, y=temp$T,col="black")
axis(side=1, at=seq(1, 365, 7))
axis(side=2, at=seq(round(min(temp$T), 1), round(max(temp$T), 1), by=2))
box()
```

 Now we can clearly see the periodic character of the time series which corresponds to the 
 seasonal changes in this latitude. For more details about the particular measurements we might
 calculate the basic characteristics of the set of values:

```{r}
stat <- summary(temp$T)
stddev <- sd(temp$T, na.rm=TRUE)
v <- as.numeric(c(stat[1],stat[6],stat[4],stat[3],stddev))
names(v) <- c("Min.","Max.","Mean","Median","Std.Dev.")
v
```

 We see that the minimum temperature does not drop far below zero even during the winter, 
 possibly due to warm currents in the northern Atlantic Ocean.
 Aside from local fluctuations, the same pattern can be expected to appear in the next 
 year, and in the year after that. Perhaps after several decades we might see a change in the 
 annual average temperature due to global warming, for instance.
 
 -----------------------------------------------------------------------------------------
   
## 2 Time Series Decomposition: Trend & Seasonal Components ##

 Now we proceed to make the first step in the analysis of the extracted temperature data. 
 It is more or less clear that we will not be able to observe any linear or exponential trends
 on such small dataset. 
 We can verify the absence of a trend by testing the series with the Mann-Kendall rank test:

```{r}
suppressMessages(pkgTest("randtests"))
randtests::rank.test(temp$T)
```

 Which suggests that there is no significant trend in the sample as a whole. We will return to the test in 
 section 3. 

 The only clearly visible systematic components are the periodic seasonal
 changes due to Earth's tilt with respect to the Ecliptic plane. 
 Clearly, the most visible cycle will repeat with a period of 365 days. Other fractions of this cycle 
 might be present as well. The remaining cycles can be observed by examining the ACF (Autocorrelation Function)
 which can be plotted using just the `acf` command:

```{r basicACFPlot2, fig.width=9, fig.height=4}
par(mfrow=c(1,2))
acf(temp$T, lag.max=365, main="Daily temp ACF")
acf(temp$T, lag.max=365, type="partial",main="Daily temp PACF")
```

 The second plot shows a partial autocorrelation function which also accounts for lags 
 shorter than the given lag $k$, that is: $k-1$ , $k-2$, ... , $2$, $1$.
 With common sense, one deduces that the data already has a 365-day cycle, but from the
 correlogram we see that other smaller cycles are present in the time series as well. 
 Hence we model the cycles with the following periods

```{r}
n <- length(temp$T)
seasons <- c(365, 365/2, 365/4, 365/12)
names(seasons) <- c("S1.","S2.","S3","S4")
seasons
```

 We may assume that there will be a month-long cycles which correspond to the rotation of the Moon, rather
 than the calendar we use:

```{r monthlyPlot, fig.width=10, fig.height=3}
months = c(
  "January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"
)
mdays = c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)
from = 1; to = 0;

par(mfrow=c(1,2))
for (month in 1:12) {
  to = ifelse(month == 12, n, to + mdays[month])
  seg = from:to
  plot(x=1:length(seg), y=temp$T[seg], type="l",
       main=paste("12:00 Temp. in ", months[month]), sub=paste("from day", from, " to ", to), xlab="[day]", ylab="T[°C]", lwd=2, lty=1.5)
  from = to + 1
}
```

The monthly data do not seem to have a repeating pattern. Hence we will only consider the annual 365-day period.
 
From now on we will separate our time series into a test part and an evaluation part. 
All regression analysis will be performed on the test part. Predictions and their quality will
be examined on the evaluation part. Generally the test part is taken as the first 80% of the original
time series:

```{r}
nt <- floor(n * 8 / 10)
temp_test <- list()
temp_test$T <- temp$T[1:nt]
temp_test$time <- temp$time[1:nt]

temp_eval <- list()
temp_eval$T <- temp$T[(nt + 1):n]
temp_eval$time <- temp$time[(nt + 1):n]

par(mfrow=c(1,1))
plot(x=temp$time, y=temp$T, type="p", main="Test and Evaluaion Parts", xlab="time", ylab="T[°C]")
lines(x=temp_test$time, y=temp_test$T, lwd=1.5, col="blue")
lines(x=temp_eval$time, y=temp_eval$T, lwd=1.5, col="green")
legend("topleft", legend=c("test","eval"),
       col=c("blue","green"), lty=1,lwd=2 , cex=0.8)
```

Now we will take the chosen season and regress them against the test part:

```{r}
model.sCos <- lm(T ~ cos(2*pi*temp$time/seasons[1]) + sin(2*pi*temp$time/seasons[1])
                   , temp)
summary(model.sCos)
```

The summary of the regression model implies that the annual period is significant.
And now we can plot the residues after extracting the seasonal components

```{r residPlot1, fig.width=10, fig.height=4}
temp_test$sCos <- model.sCos$fitted.values[1:nt]
temp_eval$sCos <- model.sCos$fitted.values[(nt + 1):n] # fitted values for later use

temp_test$Tres <- model.sCos$residuals[1:nt]
temp_eval$Tres <- model.sCos$residuals[(nt + 1):n] # residuals for later use

par(mfrow=c(1,2))
plot(T ~ temp_test$time, temp_test,
     main="Fitted Annual Temperature Model",xlab="day",ylab="T[°C]",axes=F)
lines(temp_test$time, temp_test$sCos, col="blue", lwd=2)
axis(side=1, at=seq(1, n, 7))
axis(side=2, at=seq(round(min(temp_test$T),1), round(max(temp_test$T),1), by=2))
lines(temp_test$time, temp_test$sCos, col="blue", lwd=2)
box()

plot(x=temp_test$time, y=temp_test$Tres,
     main="Residuals",xlab="day",ylab="res(T)[°C]",axes=F,type="l")
axis(side=1, at=seq(1, n, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()
```

We can see that the residuals after extracting the seasonal components still have not 
lost some of their periodic behavior. Taking a look at the residual ACF and PACF we see 
that non-zero correlation extends from lag of approximately 50 days.

```{r ACFPlot2, fig.width=9, fig.height=4}
par(mfrow=c(1,2))
acf(temp_test$Tres, lag.max=365, main="Residual ACF")
acf(temp_test$Tres, lag.max=365, main="Residual PACF", type="partial")
```

When examining the residual ACF we notice remaining significant correlation for multiple lag
values. This is possible due to remaining cyclical components which will be found as significant
frequencies after Fourier analysis in the next section.

 ---------------------------------------------------------------------------------------
             
## 3 Time Series Decomposition: Cyclical Components & Randomness Tests on Residuals
### 3.1 Cyclical Components and Fourier Analysis on Time Series

Now we continue with another step in the decomposition of the original time series.
We separate the remaining oscillations via discrete Fourier analysis. A continuous spectrum
is generated from the ACF using the following function:

```{r SpecPlot2, fig.width=9, fig.height=4}
SpecDens <- Vectorize(  
  FUN = function(omega, acov=NULL, data=NULL) {
    if(is.null(acov)) {
      if(is.null(data)) {
        stop("Please provide either vector of autocovariance function values or time series data.")
      }
      else acov <- acf(data, type="covariance", plot=FALSE)
    }
    k <- seq(to=length(acov)-1)
    ( acov[1] + 2*sum(acov[-1]*cos(k*omega)) ) / (2*pi)
  },
  vectorize.args = "omega"
)


par(mfrow=c(1,2))
temp_test.ACF <- as.numeric( acf(temp_test$Tres, type="covariance", plot=FALSE)$acf )
plot(function(x) SpecDens(x, acov=temp_test.ACF), from=2*pi/(nt),
     to = 2 * pi / 4, xlab="frequency", ylab="spectral density")
plot(function(x) SpecDens(2*pi / x, acov=temp_test.ACF), from=4, 
     to = nt, xlab="period", ylab="spectral density")
```

The figure on the right provides a better image of the (continuous) distribution of individual
components with periods of oscillation. The hidden frequencies are obtained using discrete
Fourier transform (more specifically Fast Fourier Transform):

```{r SpecDens,fig.width=11, fig.height=3.5}
temp_test.FFT <- abs(fft(temp_test$Tres)) # Fourier transform
omega <- 2 * pi * seq(0, nt / 2 - 1) / nt
period <- 2 * pi / omega
par(mfrow=c(1,1))
plot(period, temp_test.FFT[seq_along(period)], main="Residual Spectral Density", 
     ylab="density", xlab="period (days)", type="h", log="x")
```

which we can then sort by density, and display the ones with the most weight:

```{r}
spectrum <- data.frame(f=temp_test.FFT[seq_along(omega)], omega=omega, period=period)
spectrum <- spectrum[order(spectrum$f, decreasing = TRUE), ]
head(spectrum, 10)
```

One reliable way to find the most significant frequencies is by using Fischer's Periodicity Test:

```{r}
signif <- cbind(spectrum[0,], 
                  data.frame(test.stat = numeric(0), crit.val = numeric(0)))
spec <- spectrum
repeat {
  test <- data.frame(test.stat = spec$f[1]/sum(spec$f),
                     crit.val = 1 - (0.05/nrow(spec))^(1/(nrow(spec)-1)))
  if(test$test.stat > test$crit.val) {
    signif <- rbind(signif, cbind(spec[1,],test));
    spec <- spec[-1,]
  } else break
}
rm(spec, test)
signif
```

As it appears, Fischer's Test yields no significant frequency, but since we observe a sequence of significant frequencies that correspond with the ACF shown earlier. Nonetheless, we assume that, say, the first 10 might contribute to the oscillatory behavior of the time series. Thus we take:

```{r}
( signif <- head(spectrum, 10) )
```

which we can then determine more precisely (by finding their local maxima) using the continuous spectral density:

```{r}
newsignif <- sapply(
  signif$omega,
  function(x) optimize(SpecDens, 
                       interval = x + c(1,-1) * pi / nt, 
                       acov = temp_test.ACF, 
                       maximum = T  )$maximum)
( newSignifs <- cbind(signif, data.frame(omega_sm = newsignif, 
                                       period_sm = 2 * pi / newsignif)) )
```

We can try to complete the regression using the first 3 periods with their fractions, but as it turns out, only the first one with its half is significant.

```{r}
( periods <- head(newSignifs, 3)$period_sm )

model.cCos <- lm(Tres ~ cos(2*pi*temp_test$time/periods[1]) + sin(2*pi*temp_test$time/periods[1]) +
                     cos(2*pi*temp_test$time/periods[1]*2) + sin(2*pi*temp_test$time/periods[1]*2) #+
                   #cos(2*pi*temp_test$time/periods[2]) + sin(2*pi*temp_test$time/periods[2]) +
                   #cos(2*pi*temp_test$time/periods[3]) + sin(2*pi*temp_test$time/periods[3])
                   ,
                 temp_test)
summary(model.cCos)
```

The cyclical components defined by first two periods seem to be significant, but we cannot be sure if they truly
belong into the systematic model. For that reason we will save the residues of this model so that we can compare its
predictive properties with only `sCos` in section 5.

```{r cyclicalRegress,fig.width=10, fig.height=4}
temp_test$cCos <- model.cCos$fitted.values


par(mfrow=c(1,1))
plot(x=temp_test$time, y=temp_test$Tres,
     main="Temperature (Residues)",xlab="day",ylab="T[°C]",axes=F)
axis(side=1, at=seq(1, nt, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
lines(x=temp_test$time, y=temp_test$cCos, col="blue", lwd=2)
box()
```

We can combine the seasonal and cyclical components into a single model:

```{r finalRegress,fig.width=10, fig.height=5}
model.SCcos <- lm(T ~ cos(2*pi*temp$time/seasons[1]) + sin(2*pi*temp$time/seasons[1]) +
                        cos(2*pi*temp$time/periods[1]) + sin(2*pi*temp$time/periods[1]) +
                        cos(2*pi*temp$time/periods[1]*2) + sin(2*pi*temp$time/periods[1]*2),
                      temp)
summary(model.SCcos)

temp_test$SCcos <- model.SCcos$fitted.values[1:nt]
temp_eval$SCcos <- model.SCcos$fitted.values[(nt + 1):n]


par(mfrow=c(1,1))
plot(T ~ temp_test$time, temp_test,
     main="Modeling Seasonal and Cyclical Components",xlab="day",ylab="T[°C]",axes=F)
lines(temp_test$time, temp_test$sCos, col="red", lwd=2)
lines(temp_test$time, temp_test$SCcos, col="blue", lwd=2)
axis(side=1, at=seq(1, nt, 7))
axis(side=2, at=seq(round(min(temp_test$T), digits=1), round(max(temp_test$T), digits=1), by=2))
box()
legend("topleft", legend=c("seasonal","seasonal + Fourier"),
       col=c("red","blue"), lty=1,lwd=2 , cex=0.8)
```

```{r finalResidues,fig.width=10, fig.height=3.5}
#length(temp_test$Tres)
#length(model.sCos$residuals)

temp_test$Tres <- model.sCos$residuals[1:nt]
temp_test$Tres2 <- model.SCcos$residuals[1:nt] # for future use

temp_eval$Tres2 <- model.SCcos$residuals[(nt + 1):n]


plot(x=temp_test$time, y=temp_test$Tres, type="l",
     main="Residuals After Seasonal",xlab="day",ylab="T[°C]",axes=F)
axis(side=1, at=seq(1, nt, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()
plot(x=temp_test$time, y=temp_test$Tres2, type="l",
     main="Residuals After Seasonal & Cyclical",xlab="day",ylab="T[°C]",axes=F)
axis(side=1, at=seq(1, nt, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()
```

 ---------------------------------------------------------------------------------------

### 3.2 Radomness Tests on Residuals After Extracting Systematic Components

Even though the original temperature time series has no trend, the residuals of the resultant time series with extracted both seasonal (and cyclical) components, that is: the systematic components might still contain some cyclical components. To verify that the residuals are a trajectory of a (stationary) stochastic process we use the, so called, "Randomness tests":
 
 1.) Durbin-Watson autocorrelation test
 
 2.) Zero ACF test
 
 3.) Signed Rank Test 
 
 4.) Spearman Rho Test
 
 5.) Turning Point Test 
 
 6.) Median Test 
 
 
We begin by examining the residues after extracting all the systematic components in `model.sCos`, as well as their ACF:

```{r resPlot1, fig.width=10, fig.height=5}
par(mfrow=c(1,2))
plot(x=temp_test$time, y=temp_test$Tres,
     main="Temperature",xlab="day",ylab="T[°C]",axes=F, type="l")
axis(side=1, at=seq(1, 356, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()

acf(temp_test$Tres, lag.max=365, main="Residual ACF")
```

As we can clearly see, the residues still have a hidden oscillatory component with a period of around 30 days. Hence we move ahead to test their randomness via the given tests.
 
#### 3.2.1 Durbin-Watson Test

The null hypothesis of the D-W test states that the residues of a time series after a least squares regression are uncorrelated, which is put against the alternative hypothesis: that the residuals follow a 1st order autoregressive (AR) process (see section 5.). 

We can either use the inbuilt function in the "car" package:

```{r}
suppressMessages(pkgTest("car"))
car::durbinWatsonTest(temp_test$Tres)
```

or write our own function:

```{r}
# only unique values in the time series will be considered
values1 <- as.numeric(temp_test$Tres)
values2 <- unique(values1)

nv <- length(values2)
den <- sum(values2 ^ 2)
( DW <- (sum((values2[2:nv] - values2[1:(nv - 1)])^2))/den )
```

The resulting DW statistic is then compared with the D-W critical values for a sample of given size and a number `k` of the terms of linear regression (intercept included), in our case `k = 2` and `size = 277`, hence in a 5% confidence interval:

```{r}
# dL         dU
# 1.79306  1.80792 (n = 270)
# 1.79690  1.81123 (n = 280)

dL <- 0.5 * (1.79306 + 1.79690)
dU <- 0.5 * (1.80792 + 1.81123)

if(DW <= dL) {
  message("Alternative Hypothesis: Positive Autocorrelation!")
} else if (4 - dL < DW && DW < 4) {
  message("Alternative Hypothesis: Negative Autocorrelation!")
} else if (dU < DW && DW < 4 - dU) {
  message("Null Hypothesis: No Autocorrelation!")
}
```

We can obtain p-values using the model matrix of the `Scos` model, simulating the stochastic process $Y \sim \rho X - \mu$, computing a DW-statistic for each simulation, and counting the number of `DW < simDW`:

```{r}

X <- model.matrix(model.sCos)[1:nt,]

reps = 1000
sig <- var(values2)
mu <- temp_test$sCos
Y <- matrix(rnorm(nv*reps, 0, sig), nv, reps) + matrix(mu, nv, reps)
E <- residuals(lm(Y ~ X - 1))
simDW <- apply(E, 2, function(e) (sum((e[2:length(e)] - e[1:(length(e) - 1)])^2) / sum(e ^ 2)) )

( p_val <- (sum(DW > simDW)) / reps ) # for negative correlation
( p_val <- (sum(DW < simDW)) / reps ) # for positive correlation
( p_val <- 2*min(p_val, 1 - p_val) ) # for two-sided correlation
```

The `rho` parameter appearing in the test is the, so called, autocorrelation coefficient for which $-1 < \rho < 1$ and the null hypothesis translates to verifying that $\rho = 0$ for some confidence interval. 
 
We can model correlation for other lags as well. Aside from the residual vector as an argument, the `durbinWatsonTest`
can process the entire regression model. It also has a `max.lag` argument which tells the function which lag to test for when using a model $Y \sim X - L$ (where $L$ is the lag). Parameter `alternative` also tells the function which type of correlation to test for:

```{r}
car::durbinWatsonTest(model.sCos, max.lag=10, alternative="two-sided")
car::durbinWatsonTest(model.sCos, max.lag=10, alternative="negative")
car::durbinWatsonTest(model.sCos, max.lag=10, alternative="positive")
```

We see that DW tests have zero p-value up to lag 4. This means that we could be dealing with an AR process of order 4 or higher.

#### 3.2.2 Zero ACF Test
The test reduces to just finding all the ACF lags k with $ACF(k) > 2/ \sqrt{n}$ where $n$ is the length of the time series.

```{r acfPlot1, fig.width=11, fig.height=5}
par(mfrow=c(1,1))
ACF <- acf(temp_test$Tres, lag.max=nt, plot=F)
lags <- which(abs(acf(temp_test$Tres, lag.max=nt, main="Residual ACF")$acf[-1]) > 2/sqrt(nt))
lagValues <- numeric()
for(i in 1:length(lags)) {
  lagValues[i] <- ACF$acf[lags[i] + 1]
  segments(lags[i], 0, lags[i], lagValues[i], col= "red", lwd=2)
}
lags
```

The result shows that the residual ACF exceeds the zero-value for the lags shown above.

#### 3.2.3 Signed Rank Test

This is a non-parametric test for the presence of a trend in the residual time series, based on the analysis of differences of sets of three consecutive terms. The null hypothesis states that the resulting statistic is asymptotically normally distributed. using a custom approach:

```{r}
# consecutive values with zero difference have to be omitted
dist_bool <- c(T,as.logical(diff(values2)))
Tt <- c()
for (i in 1:length(dist_bool)) {
  if (dist_bool[i]) Tt <- c(Tt, values2[i])
}

#now the ranking of differences
ranks <- c()
for (i in 2:length(Tt)) {
  if (Tt[i-1] < Tt[i]) {
    ranks[i - 1] <- 1
  } else {
    ranks[i - 1] <- 0
  }
}

rank_sum <- sum(ranks)
rank_mean <- (length(Tt) - 1) / 2
rank_var <- (length(Tt) + 1) / 12
test_stat <- (rank_sum - rank_mean) / sqrt(rank_var)

alpha = 0.05

if (test_stat < qnorm(alpha) || test_stat > qnorm(1 - alpha)) {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Alternative Hypothesis! test_stat is not normally distributed for n-->infty ! Trend detected."))
} else {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Null Hypothesis! test_stat is normally distributed for n-->infty ! No trend detected."))
}
```

And we can compare the results with an inbuilt function from the `randtests` package:

```{r}
require(randtests)
randtests::difference.sign.test(temp_test$Tres)
```

So as it appears, for a 5% confidence interval the residues still contain a trend. Substituting 0.01 for `alpha`,however, gives an alternative hypothesis as a result. Thus the signed-rank test verifies its null hypothesis for $0.1 > \alpha > 0.05$. This is probably caused by cutting off a portion of the data at the end of an annual sample.

#### 3.2.4 Spearman Rho Test

Another non-parametric test for the presence of a trend using concordant/discordant pairs of values can be implemented as:

```{r}
#order the unique values in an increasing order

temp_ordered <- values2[order(values2, decreasing=F)]
q <- numeric()

for (i in 1:length(values2)) {
  q[i] <- match(c(values2[i]), temp_ordered) 
}

# and for the spearman rho coefficient
nv <- length(values2)
sum_term <- numeric()
for (i in 1:length(values2)) {
  sum_term[i] <- (i - q[i])^2 
}

rho <- 1 - (6 / (nv * (nv * nv - 1))) * sum(sum_term)

test_stat <- abs(rho) * sqrt(nv - 1)

if (test_stat <= qnorm(alpha) || test_stat >= qnorm(1 - alpha)) {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Alternative Hypothesis! test_stat is not normally distributed for n-->infty ! Trend detected."))
} else {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Null Hypothesis! test_stat is normally distributed for n-->infty ! No trend detected."))
}
```

And using the inbuilt `cor.test`:

```{r}
# our rho:
rho

cor.test(temp_test$time, temp_test$Tres, method="spearman")
```

Clearly both the Signed-Rank, and Spearman Rho tests show that the residual time series is a realisation of independent identically distributed random variables.

#### 3.2.5 Turning Point Test

This and the following test are both tests for the presence of (previously unfiltered) periodic components in the  residual time series. Similarily to the signed rank test, it examines sets of three consecutive terms, except this time looking for so called 'turning points', that is: triplets of values in which the middle value is locally extremal (in that triplet). The presence and periodic regularity of turning points implies oscillatory behavior or the residues.

```{r}
dist_bool <- c(T,as.logical(diff(values2)))

#removing duplicate consecutive values
Tt <- c()
for (i in 1:length(dist_bool)) {
  if (dist_bool[i] == TRUE)  Tt <- c(Tt, values2[i])
}

# marking consecutive triplets with value 0 for upper and lower turning points
# and 1 otherwise

Mt <- c()
for (i in 2:(length(Tt) - 1)) {
  if ( ((Tt[i - 1] < Tt[i]) && (Tt[i] > Tt[i + 1])) || 
       ((Tt[i - 1] > Tt[i]) && (Tt[i] < Tt[i + 1])) ) {
    Mt[i - 1] <- 1
  } else {
    Mt[i - 1] <- 0
  }
}

M_sum <- sum(Mt)
M_mean <- 2 * (length(Tt) - 2) / 3
M_var <- (16 * length(Tt) - 29) / 90

test_stat <- (M_sum - M_mean) / sqrt(M_var)

if (test_stat <= qnorm(alpha) || test_stat >= qnorm(1 - alpha)) {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Alternative Hypothesis! Periodic components detected."))
} else {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Null Hypothesis! No periodic components detected."))
}

( p_value <- pnorm(test_stat) ) # positive serial correlation
( p_value <- 2*min(p_value, 1 - p_value)) # (two-sided) non-randomness
( p_value <- 1 - p_value ) # negative serial correlation
```

And via the inbuilt function from `randtests`:

```{r}
randtests::turning.point.test(temp_test$Tres, alternative="left.sided")
randtests::turning.point.test(temp_test$Tres, alternative="two.sided")
randtests::turning.point.test(temp_test$Tres, alternative="right.sided")
```

we, apparently, obtain the same result. 
 
#### 3.2.6 Median Test

This is another non-parametric test for the presene of periodic components which essentially divides the sample values into distinct groups each of which corresponds to a group of values that are unilaterally above or below the sample median, meaning that if the $i$-th value and its predecessors lie above the median, and the $(i+1)$-th value lies below, for instance, the $(i+1)$-th value is a member of a new group of values which lie below the sample median.

```{r medianPlot1, fig.width=11, fig.height=5}
U <- temp_test$Tres
temp_med <- median(U)
U_ <- U - temp_med
U_ <- U_[U_ != 0]
M <- U_ > 0
head(as.numeric(M), 10)

c(below=sum(diff(M)<0), above=sum(diff(M)>0))

m <- sum(M)
P <- sum(diff(M)>0) + sum(diff(M)<0) + 1

par(mfrow=c(1,1))
plot(temp_test$Tres ~ temp_test$time, main="Median", ylab="res")
abline(h=temp_med, col="green")
for(i in 1:length(temp_test$Tres)) {
  if (U[i] != temp_med ) {
    if (U[i] > temp_med) {
      segments(temp_test$time[i], temp_med, temp_test$time[i], U[i], col= "red", lwd=1)
    } else {
      segments(temp_test$time[i], temp_med, temp_test$time[i], U[i], col= "blue", lwd=1)
    }
  }
}

( ZStatistic <- (P - (m+1)) / sqrt(m*(m-1)/(2*m-1)) )


if (ZStatistic > qnorm(alpha / 2) && ZStatistic < qnorm(1 - alpha/2)) {
  message(paste("qnorm(",alpha/2,") < Zstat < qnorm(",1 - alpha/2,")"))
  message("", round(qnorm(alpha / 2), digits=2)," < ", round(ZStatistic, digits=2)," < ",round(qnorm(1 - alpha/2), digits=2),"")
  message("Null Hypothesis! Randomness!")
  if (m <= 100) cat("Note: 'above median' group count: m = ", m," <= 100")
} else {
  message(paste("qnorm(",alpha/2,") <= Zstat || Zstat >= qnorm(",1 - alpha/2,")"))
  message("", round(qnorm(alpha / 2), digits=2)," <= ", round(ZStatistic, digits=2)," >= ",round(qnorm(1 - alpha/2), digits=2),"")
  message("Alternative Hypothesis! Non-Randomness ---> Periodicity!")
  if (m <= 100) cat("Note: 'above median' group count : m = ", m," <= 100")
}

( p_value <- pnorm(ZStatistic) ) # positive serial correlation
( p_value <- 2*min(p_value, 1 - p_value)) # (two-sided) non-randomness
( p_value <- 1 - p_value ) # negative serial correlation

randtests::runs.test(U)

# library(signmedian.test) # NOTE: signmedian.test does not do the same thing as the median test above
# signmedian.test(temp_test$Tres, alternative="two.sided")
# signmedian.test(temp_test$Tres, alternative="greater")
# signmedian.test(temp_test$Tres, alternative="less")
```

The median test implies randomness of the residual time series, yet its results are irrelevant for group count $m \leq 100$. The result of the Durbin-Watson Test implies that the residual time series may be a trajectory of an $AR(3)$
(auto-regressive) process, and the residual ACF, of course, shows non-zero correlation for multiple lags. The signed-rank
test "almost" implies that the residues do not have a trend (more specifically, for $\alpha = 0.1$), and the following
Spearman rho test does so as well. The presence of a trend may be caused by cutting off the tail of the annual series.
The Turning Point test, with the Median test which  followed show that the time series also contains unfiltered periodic
components, even though the Median test may be inconclusive since the group count m of groups of 
values above median is less than 100.
 
Therefore, from all the tests carried out in this section, we can conclude the following:
 
 - The residual time series may be a trajectory of an $AR(3)$ (auto-regressive) process
 - There is no trend present in the residual time series
 - And finally the unfiltered periodic components have to be accounted for via other methods


 ---------------------------------------------------------------------------------------------------------------
 
## 4. Introducing ARMA Models and the Hannan-Rissanen Procedure

In general, a stationary (only lag-dependent) stochastic process is determined by p auto-regressive and q moving-average terms. The auto-regressive terms (as by the name) determine the value of the stochastic process at each time from its past values from up to p steps back, and the moving-average terms on the other hand imply dependence on a random process, such as the white noise, for instance, up to q time steps back. 

```{r newRegressionAcf, fig.width=12, fig.height=5}
par(mfrow=c(1,3))
plot(x=temp_test$time, y=temp_test$Tres, main="Residues",xlab="day",ylab="T[°C]",axes=F, type="l")
axis(side=1, at=seq(1, 365, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()

ACF <- acf(temp_test$Tres, lag.max=365, plot=F)
n<-length(temp_test$Tres)
lags <- which(abs(acf(temp_test$Tres, lag.max=365, main="Residual ACF")$acf[-1]) > 2/sqrt(n))
lagValues <- numeric()
for(i in 1:length(lags)) {
  lagValues[i] <- ACF$acf[lags[i] + 1]
  segments(lags[i], 0, lags[i], lagValues[i], col= "red", lwd=2)
}
acf(temp_test$Tres, lag.max=365, main="Residual PACF", type="partial")
```
 
Based on the result of the Durbin-Watson test, the additional oscillations (which can still be seen in red non-zero values of the last residual ACF) should be accounted for by modelling the residues by a suitable $AR(p)$ model (or $ARMA(p,q)$ in general). The process for determining the proper orders $p$ and $q$ of suitable ARMA model candidates is called the Hannan-Rissanen Procedure and it consists of multiple steps.

First, we need to figure out whether the residues need additional transformation, so that they could be properly modelled. For that we test whether:
 (a) the time series has zero mean value
 (b) the time series is stationary

```{r}
mean(temp_test$Tres)
```

So the mean value approximation is close to zero, and the stationarity can be tested using the following test:

```{r}
suppressMessages(pkgTest("tseries"))
adf.test(temp_test$Tres, alternative="explosive") # Augmented Dickey-Fuller
```

According to the Augmented Dickey-Fuller tests the residuals the stationarity hypothesis is rejected. However, since The tests have low statistical power in that they often cannot distinguish between true unit-root processes and near unit-root processes. This is called the "near observation equivalence" problem.

### 4.1. The Hannan-Rissanen Procedure
 
First, we need to set the maximum values for lag parameters based on the significant lags in residual ACF and PACF. It will be easier to do so in a combined plot:

```{r plot03, echo=T, fig.width=8, fig.height=4}
par(mfrow=c(1,1))
acf(temp_test$Tres, ylab="(P)ACF", lag.max=100, main="Comparison of Residual ACF & PACF")
tmp <- pacf(temp_test$Tres, plot=F, lag.max=100)
points(tmp$lag+0.3, tmp$acf, col="red", type = "h")
legend("topright", legend=c("ACF","PACF"), col = c("black","red"), lty=c(1,1))
```

The partial correlogram (red) shows that the most significant correlations are for lags $L = 1$, and $L = 40$. The second lag, however, does not exceed the zero region nearly as much as $L = 1$. It is then reasonable to assume that the most viable model will be of AR order $p = 1$. Hence:

```{r}
kmax = 90;
```

The Yule-Walker method based on solving regression equations against an increasing basis of AR terms can be done through the inbuilt `ar()` function which automatically finds the model with the lowest AIC (Akaike's Information Criterion). And by plotting the `$aic` parameter we obtain differences $AIC_{min} - AIC_k$ for all models.

```{r}
model <- list()
model$ar.yw <- ar(temp_test$Tres, order.max=kmax)
```

Plotting the differences we notice that the highest drop in $\Delta AIC$ comes after $L=1$ and then a much smaller drop follows after `L=15` and `L=40` after that. We can determine the maximum order $k$ from the lags where this drop in $\Delta AIC$ occurs. An alternative to this approach is determining the AR order $p$ from the residual variances:

```{r plot05, echo=T, fig.width=9.5, fig.height=4}
par(mfrow=c(1,2))
plot(0:(length(model$ar.yw$aic)-1), xlab="p", model$ar.yw$aic, ylab="dAIC", main="Differences in AIC")
rbind(coef=model$ar.yw$ar, se=sqrt(diag(model$ar.yw$asy.var.coef))) 
tmp <- sapply(1:kmax, function(x) ar(temp_test$Tres, aic=F, order.max=x)$var.pred)
plot(tmp, xlab="p", ylab="sigma^2", main="Residual variances")
```

The plot suggests that the highest order of the AR process should be around `p = 20`. The maximum AR order can also be determined via a recursive Lewinson-Durbin algorithm. For the sake of saving computation time, however, we choose the following maximum order parameters:

```{r}    
kmax = 20; pmax = 10; qmax = 10;

#LongAR method implementation:
LongAR = function(tser, k, p, q) {
  if (!is.ts(tser)) {
    tser = ts(tser)
  }
  
  tt <- data.frame(x=tser)
  tt$z <- ar.ols(tt$x, aic=F, order.max=k)$resid
  tt$z[is.na(tt$z)] <- 0
  
  # suppressMessages(library(dynlm))
  
  if (p > 0 & q > 0)  outmodel <- dynlm(x ~ L(x, 1:p) + L(z, 1:q), tt)
  else if (p > 0 & q == 0) outmodel <- dynlm(x ~ L(x, 1:p), tt)
  else if (q > 0 & p == 0) outmodel <- dynlm(x ~ L(z, 1:q), tt)
  else warning("LongAR::error! invalid order p,q")
  
  outmodel
}

# a method for returning model's AIC (Akaike's Information Criterion) and BIC (Bayesian Information Criterion)
# with a changeable parameter
InfCrit = function(ml_model, type="AIC") {
  if (type=="AIC") {
    AIC(ml_model)
  } else if (type=="BIC") {
    BIC(ml_model)
  } else {
    warning("Invalid type (AIC or BIC)", call=F)
  }
}


#determine the model's AIC using an inbuilt function ar()

AICs <- ar(temp_test$Tres, order.max=kmax)$aic

# find minimal AIC for pmax <= p <= kmax
minAIC <- min(AICs[pmax:kmax])
for (k in pmax:kmax) {
  if (AICs[k] == minAIC) {
    korder <- k
    break
  }
}

( k <- korder )

models.arma <- list() 

suppressMessages(pkgTest("dynlm"))
models.arma[[paste(1,0,sep=",")]] <- LongAR(temp_test$Tres, k, 1, 0)
models.arma[[paste(0,1,sep=",")]] <- LongAR(temp_test$Tres, k, 0, 1)

for(p in 1:pmax) {  
  for(q in 1:qmax) {
    models.arma[[paste(p,q,sep=",")]] <- LongAR(temp_test$Tres, k, p, q)
  }
}

bic <- cbind(
  BIC = sapply(models.arma, function(x) InfCrit(x, type="BIC")),
  AIC = sapply(models.arma, function(x) InfCrit(x, type="AIC"))
)
bic <- bic[order(bic[,"BIC"]),]

head(bic, n=5)
```

Now we have 5 ARMA models with the lowest BIC with orders `"p,q"`.

```{r}
bestArma <- head(bic, n=5)
orders <- rownames(bestArma)
topModels <- list() # here I put the top models marked by their orders "p,q" as a key

for (j in 1:length(orders)) {
  key <- orders[[j]]
  topModels[[key]] <- models.arma[[key]]
}

( bestArma <- cbind(bestArma, topModels) ) #checking if the list contains models
#and then accessing them in the following way:
bestArma[[1,3]] #first in the list and the third column for the model itself
```

### 4.2. Adjusting the Regression Coefficients Using the Maximum Likelihood Estimate
 
The Maximum Likelihood Estimate (MLE) optimizes the, so called, likelihood-function with respect to the regression coefficients. One can find estimates using the Residual Square Sum (RSS). We will optimize the model parameters via an inbuilt function arima, specifying parameter `method = "ML"` and also using the former `dynlm`-type model and directly calculating the RSS.

```{r}
MaxLikelihoodOptim = function (tmpmodel, use_arima=TRUE) {
  params <- names(tmpmodel$coefficients)
  ( p <- sum(grepl("x",params)) )
  ( q <- sum(grepl("z",params)) )
  
  pars0 <- tmpmodel$coefficients
  
  #inbuilt function
  if (use_arima) {
    coefs <- arima(temp_test$Tres, order = c(p, 0, q), 
                   init=c(pars0[-1], pars0[1]), method = "ML", transform.pars = F)$coef
    
    # making the intercept coefficient first in the list
    intersect <- coefs[length(coefs)]
    coefs <- coefs[1:(length(coefs)-1)]
    coefs <- c(intersect, coefs)
    
    return(coefs)
  } else {
    #other version
    qmp <- max(0,q - p)
    x <- as.numeric(temp_test$Tres)
    ntest <- length(x)
    
    #residual square sum
    RSS = function(pars) {
      z <- rep(0,length(x) + qmp)
      for(i in (p + 1):ntest) {
        xz <- c(1, x[i:(i - p)], z[(i:(i - q)) + qmp])
        z[i + qmp] <- x[i] - c(append(head(pars, n = p + 1), 0, after = 1), 0, tail(pars, n = q)) %*% xz
      }
      z <- z[-(1:(p + qmp))]
      sum(z^2)
    }
    
    optim(pars0, RSS)$par
  }
}

#model comparison function
CompareModels = function(models) {
  orders <- rownames(models)
  
  output <- list()
  
  for (i in 1:length(orders)) {
    HannanRissanen_Result <- models[[i,3]]$coefficients
    MLE_using_ARIMA <- MaxLikelihoodOptim(models[[i,3]])
    MLE_not_ARIMA <- MaxLikelihoodOptim(models[[i,3]], use_arima=F)
    output[[ orders[[i]] ]] <- cbind(HannanRissanen_Result, MLE_using_ARIMA, MLE_not_ARIMA)
  }
  
  output
}

( comparison <- CompareModels(bestArma) )

#changing model coefficients
OptimizeModels = function (models, use_arima=F) {
  orders <- rownames(models)
  
  for (i in 1:length(orders)) {
    if (use_arima) {
      result_coeffs <- MaxLikelihoodOptim(models[[i,3]])
    } else {
      result_coeffs <- MaxLikelihoodOptim(models[[i,3]], use_arima=F)
    }
    
    models[[i,3]]$coefficients <- result_coeffs
    
  }
  models
}

newBestArma <- OptimizeModels(bestArma, use_arima=F)

# checking if the coefficients were changed
bestArma[[5,3]]$coefficients
newBestArma[[5,3]]$coefficients
```

### 4.3. Plotting the Resulting Models With their Residuals

```{r plot11csaa, echo=T, include=T, fig.width=9, fig.height=6}
par(mfrow=c(2,2))

for (i in 1:5) {
  plot(temp_test$Tres, type="p", main=paste("ARMA(",orders[[i]],")"), ylab="T")
  lines(newBestArma[[i,3]]$fitted.values, col="blue")
  plot(newBestArma[[i,3]]$residuals, main=paste("ARMA(",orders[[i]],") residuals"), ylab="res")
}
```

### 4.4. Plotting the Resulting Models In the Original Time Series

```{r plot1212, echo=T, fig.width=8, fig.height=4}
par(mfrow=c(1,1))
n <- length(temp_test$Tres)
for(i in 1:5) {
  o <- unlist(strsplit(orders[[i]],",")); p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]])
  m <- length(newBestArma[[i,3]]$fitted.values)
  plot(temp$T[1:m], type="p", main=paste("Systematic lm + ARMA(",orders[[i]],")"), xlab="day",ylab="T")
  
  syst_fit <- model.sCos$fitted.values[1:m]
  lines(syst_fit, col="red", lwd=2)
  lines(newBestArma[[i,3]]$fitted.values + syst_fit, col="blue", lwd=2)
  legend("topleft", legend=c("(1): Seasonal", paste("(2): (1) + ARMA(",orders[[i]],")")),
         col=c("red","blue"), lty=1,lwd=2 , cex=0.75)
}
```

The Hannan-Rissanen procedure with maximum orders: `kmax = 20`, `pmax = 10`, and `qmax = 10`, determined the optimal ARMA models to be: 

```{r, echo=F}
row.names(newBestArma)
```
Combining the systematic components, i.e.: the seasonal and cyclical components, with the linear regression of a given ARMA model we have found 5 most accurate linear models for the test part of the original time series. The evaluation part will be used in the following section where we will test the given models and carry out predictions.
 
## 5. ARMA Model Diagnostics and Predictions
 
Prior to carrying out (single-step and multiple step) predictions, we need to test the accuracy of the given models. For that we use the following series of diagnostic tests:
 - Zero Autocorrelation
 - Normality of Residues
 - Conditional Heteroskedasticity
 
### 5.1. Zero Autocorrelation
 
The Autocorrelation function (ACF) shows a reasonably clear image of the dependency of individual random variables with respect to time lag. This means that if the ACF has non-zero values for some lags greater than zero, then it is likely that the residues after removing the ARMA regression's fitted values still contain some systematic or ARMA component. To find out we test for a null hypothesis: $ACF(k) = 0$ for all $k > 0$, against an alternative $ACF(k) \neq 0$ for some $k > 0$.

```{r plot1zaa, echo=T, fig.width=9, fig.height=4}
ZeroACF = function (x, zeroValue) {
  x = ifelse (abs(x) < zeroValue, 0, x)
}

NonZeroValues = function (series, zero) {
  indices <- which(series != 0)
  nonzeros <- list()
  
  nonzeros[["zero val"]] <- zero
  
  for (i in 2:length(indices)) {
    nonzeros[[paste("k",indices[i], sep = '_')]] <- series[indices[i]]
  }
  
  data.frame(head(nonzeros))
}

ACFs <- list()
Nonzeros <- list()

par(mfrow=c(1,2))
for (i in 1:5) {
  acf(newBestArma[[i,3]]$residuals, ylab="ACF", lag.max=nt, main=paste("ARMA(",orders[[i]],") res"))
  
  ACFs[[i]] <- acf(newBestArma[[i,3]]$residuals, ylab="ACF", lag.max=nt, plot=F)$acf
  zero <- 2 / sqrt(nt)
  ACFs[[i]] <- sapply(ACFs[[i]], function(x) ZeroACF(x, zero))
  lines(ACFs[[i]], col="red", lwd=2)
  
  Nonzeros[[i]] <- NonZeroValues(ACFs[[i]], zero)
}
```

Using the zero-value approximation: $2/\sqrt{n_t}$, where $n_t$ (`nt`) is the length of the residual time series, we notice that there seems to be a resilient 83-day lag which was not removed (even after including the significant residual lags such as 45, and 83 in the `model.sCos` in section 3). The significance of the 83-day lag, however, is not nearly as high as it would have otherwise been (with additional significant lags) if additional lags had not been introduced prior to carrying out the Hannan-Rissanen procedure. Thus we can assume that all models, having no other significant lags than the 83-day lag, describe the behavior of the test part of the residual time series reasonably well. In the following list:

```{r}
Nonzeros
```

we see the exact lags for which the ACF exceeds the zero-value.
 
### 5.2. Normality of Residues
 
If a given ARMA model describes the time series well enough, the residues should behave as if they were generated by a stochastic process with values at individual times $t$ being the realizations of independent identically distributed random variables. For sufficiently large number of time steps, the residues are assumed to be the results of a normally distributed random variable. 
 To test that, we use the, so called, Jarque-Bera test:

```{r JBTest, echo=T}
JBToutput <- list()

for (i in 1:5) {
  JBToutput[[i]] <- tseries::jarque.bera.test(newBestArma[[i,3]]$residuals)
  
  pValue <- as.numeric(JBToutput[[i]]$p.value)
  
  JBToutput1 <- c("1,0",JBToutput[[i]]$statistic, JBToutput[[i]]$parameter, pValue)
  names(JBToutput1) <- c("p,q","statistic", "DOF", "p.value")
  data.frame(head(JBToutput1))
  if (pValue > 0.05) {
    message(paste("ARMA(",orders[[i]],") ::: p-value = ", round(pValue, 4),", Normality hypothesis rejected"))
  } else { message(paste("ARMA(",orders[[i]],") ::: p-value = ", round(pValue, 4),", Normality hypothesis accepted")) }
}
```

As it appears, the normality of the residues of all top 5 ARMA models is accepted. 

### 5.3. Conditional Heteroskedasticity
 
The term "heteroskedasticity" of a time series describes the tendency of its variance to change with time. A time series whose variance (up to a given time) remains the same are called "homoskedastic". The following Breusch-Pagan test assesses the homoskedasticity of a given model:

```{r Skedast, echo=T}
alpha = 0.05
SkedtestResults <- list()
suppressMessages(pkgTest("lmtest"))
for (i in 1:5) {
  (SkedtestResults[[i]] <- lmtest::bptest(newBestArma[[i,3]]) )
  
  pValue <- as.numeric(SkedtestResults[[i]]$p.value)
  
  if (pValue > alpha) {
    message(paste("ARMA(",orders[[i]],") ::: p-value = ", pValue,", Null hypothesis rejected: model is heteroskedastic"))
  } else {
    message(paste("ARMA(",orders[[i]],") ::: p-value = ", pValue,", Null hypothesis accepted: model is homoskedastic"))
  }
} 
```

Thus on the 5% significance level we accept the null hypothesis for all models, i.e.: all models are heteroskedastic.

 -----------------------------------------------------------------------------------------------------------------

### 5.4. Forecasting and Single-Step Predictions
 
Now we make use of the evaluation part of the time series which we have separated from the original series earlier in section 4. Since the `dynlm` model we used earlier does not support `predict()` function from the `forecast` library we use equivalent $ARIMA(p,0,q)$ models instead of $ARMA(p,q)$, which are returned by an inbuilt function `arima()`, the result of which is an object that can be processed by the `predict()` call.

```{r predictplot1, echo=TRUE, fig.width=9, fig.height=4}
( neval <- length(temp_eval$Tres) )

par(mfrow=c(1,1))
suppressMessages(pkgTest("forecast"))
eval.predictions <- list()
merrors <- list()

for (i in 1:5) {
  # extract model orders from the top models from Hannan-Rissanen:
  o <- unlist(strsplit(orders[[i]],",")); p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]])
  # estimate a suitable model on the test part of the time series
  model_estim <- arima(temp_test$Tres, order=c(p,0,q))
  # use the estimated model as an argument, fitting the same model to the whole time series without re-estimating any parameters
  # taking the fitted seasonal compnent into account
  eval.predictions[[i]] <- Arima(model.sCos$residuals, order=c(p,0,q), model=model_estim, xreg=model.sCos)
  # and take the evaluation part for examination
  eval.predictions[[i]] <- tail(fitted(eval.predictions[[i]]), neval)
  merrors[[i]] <- mean((temp_eval$Tres - eval.predictions[[i]])^2)

  plot(x=temp_eval$time, y=temp_eval$T, 
       main=paste("Predictions ARMA(",p,",",q,"), RMSE = ", round(sqrt(merrors[[i]]), 4)),
       xlab="day", ylab="[°C]", type="l", lwd=1.5)
  lines(x=temp_eval$time, y=(eval.predictions[[i]] + temp_eval$sCos), col="blue", lwd=2)
  legend("bottomleft", legend=c("prediction", "actual values"),
         col=c("blue","black"), lty=1,lwd=2 , cex=0.75)
}
```

We have also computed Root Mean Square Errors for each model, with the smallest one being:

```{r}
minE <- which.min(merrors)
```

### 5.5. Comparison of Predictive Abilities Using the Diebold-Mariano Test
 
Taking two different ARMA models, the null hypothesis of the test states that both models have equal predictive abilities. We will be comparing 5 different models against each other, which will essentially amount to testing the predictive abilities of 10 distinct pairs of models. Practically, the test results can be expressed in a $5 \times 5$ "DieboldMarianoMatrix" with values $0$ (if model $i$ has the same predictive abilities as model $j$), $1$ (if model `i` has better predictive abilities than model $j$), and $-1$ (the other way around):

```{r}
DieboldMarianoMatrix <- matrix(0, ncol=5, nrow=5)
pvalueMatrix <- matrix(1, ncol=5, nrow=5)
for(i in 1:length(eval.predictions)) {
  for(j in 1:length(eval.predictions)) {
    if(i==j) next
    test <- forecast::dm.test(temp_eval$Tres - eval.predictions[[i]], temp_eval$Tres - eval.predictions[[j]])
    DieboldMarianoMatrix[i,j] <- (test$p.value < alpha) * sign(test$statistic) 
    pvalueMatrix[i,j] <- round(test$p.value, 3)
  }
}
DieboldMarianoMatrix

pvalueMatrix
```

As the results of the Diebold-Mariano test suggest, no model has significantly better predictive abilities on a 5% significance level.
A quantitative characteristic of predictive ability is the Root Mean Square Error (RMSE). Here we can see how the predictions match the observed values from the evaluation part of the time series:

```{r predictPlot2, echo=T, fig.width=9, fig.height=6}
par(mfrow=c(1,1));
o <- unlist(strsplit(orders[[minE]],",")); p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]])
n <- length(model.sCos$fitted.values)
fullpredict <- as.numeric(eval.predictions[[minE]] + temp_eval$sCos)
plot(x=temp$time, y=temp$T, main=paste("Prediction + Systematic , ARMA(", orders[[minE]],"):"), ylab="T",xlab="day")
lines(x=temp_eval$time, y=fullpredict, col="blue", lwd=2)

k = max(p, q)
syst_fit <- model.sCos$fitted.values[(k + 1):nt]
test_model <- newBestArma[[minE,3]]$fitted.values + syst_fit

lines(x=temp_test$time[(k + 1):nt], y=test_model, col="red",lwd=2)
lines(x=temp$time[nt:(nt + 1)], y=c(test_model[nt - 1], fullpredict[1]), col="red", lwd=2) # line connecting the test and eval datasets
legend("topleft", legend=c(paste("Model ARMA(",orders[[minE]],")"), paste("Model ARMA(",orders[[minE]],") prediction")),
       col=c("red","blue"), lty=1,lwd=2 , cex=0.75)
```

The model with the smallest $RMSE$ is $ARMA(1,0)$. Except for the outlying values, the predictive capabilities of the model can also be verified visually from the plot above. 
 
### 5.6 Multiple-Step Predictions

Predicting one step ahead of data can be done naiively by fitting and ARMA model on the entire time series (both test and evaluation segments). Once we reach the end of the dataset, however, we must predict with an increasing amount of uncertainty. This can be done by recursive accumulation of the past values (based on AR an MA parameters). A `predict()` method for an `arima` (or `Arima`) fit can perform this task for an arbitrary forecast horizon. The `forecast` package contains a method (with the same name) which also includes confidence levels (80% and 95% as default) enclosing the predicted values which tend towards a stable mean. The confidence intervals can grow with increasing steps, especially for models with higher MA degrees.

In this section we compare the multiple-step predictions (with the length of the evaluation part) for the top 5 ARMA models:

```{r multipredictPlot, echo=T, fig.width=12, fig.height=4}
require(forecast)
par(mfrow=c(1,1))
ne <- length(temp_eval$time)
x <- as.numeric(temp_test$Tres)
X <- as.numeric(model.sCos$residuals)

for (i in 1:5) {
  o <- unlist(strsplit(orders[[i]],","));
  p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]]);
  key <- paste("(",p,",",q,")", sep="")
  
  model_estim <- arima(x, order=c(p,0,q))
  model <- Arima(X, order=c(p,0,q), model=model_estim, xreg=model.sCos) # use the seasonal regression for xreg
  
  xp <- tail(fitted(model), ne)
  fc <- forecast(model_estim, h=ne)
  
  xp_res <- temp_eval$Tres - xp
  rmse <- sqrt(mean(xp_res^2))
  rmse_h <- sqrt(mean((fc$mean - xp)^2))
  
  fc$mean <- fc$mean + temp_eval$sCos
  fc$lower <- fc$lower + temp_eval$sCos
  fc$upper <- fc$upper + temp_eval$sCos
  fc$x <- fc$x + temp_test$sCos
  xp <- xp + temp_eval$sCos
  plot(fc, lwd=1, xlab="day", ylab="[°C]", main=paste("ARMA", key, sep=""), xlim=c(floor(0.6*length(x)), length(X)))
  mtext(paste(" RMSE(1) = ", round(rmse, digits=3), " ,  RMSE(", ne, ") = ", round(rmse_h, digits=3), sep=""))
  lines(x=(nt + 1):(nt + ne), y=xp, lwd=2, col="green4", type="l")
  lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="red", type="p", pch=20)
  legend("bottomleft", legend=c(paste("prediction (", ne,"-step)", sep=""), "1-step prediction","data"),
         col=c("blue", "green4","red"), lty=c(1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, 20))
}
```

Now even though we have no validation for our models for $t > n$ where $n$ is the length of the time series, we can use our models to make a forecast for another year (more precisely, year 2016):

```{r futurePredictPlot, echo=T, fig.width=12, fig.height=4}
  i <- 1
  o <- unlist(strsplit(orders[[i]],","));
  p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]]);
  key <- paste("(",p,",",q,")", sep="")

  # forecast for another year (2016)
  n_future = 365
  model_future <- arima(X, order=c(p,0,q))
  fc_future <- forecast(model_future, h=n_future)
  future_sCos <- sapply((nt + ne):(nt + ne + n_future - 1), 
                        function(t) c(1, cos(2*pi*t/seasons[1]), sin(2*pi*t/seasons[1])) %*% model.sCos$coefficients)
  
  fc_future$mean <- fc_future$mean + future_sCos
  fc_future$lower <- fc_future$lower + future_sCos
  fc_future$upper <- fc_future$upper + future_sCos
  fc_future$x <- fc_future$x + model.sCos$fitted.values
  
  plot(fc_future, lwd=1, xlab="day", ylab="[°C]", main=paste("ARMA", key, sep=""))
  legend("topleft", legend=c("prediction","data"), col=c("blue", "black"), lty=c(1, 1),lwd=2 , cex=0.8)
```

And unless we take global warming (a possible trend), for example, into account we can assume this prediction to be valid. Since our dataset is only a year long we are unable to make any assumptions about possible long-term changes in temperature.

 --------------------------------------------------------------------------------------------------------------
 
## 6. Non-Stationary Models
 
ARMA type models can be used when we safely assume stationarity of the time series. Namely, when the series has a constant variance its mean values and covariances only depend on the lag, not on time. Although ARMA models seem to sufficiently describe the given time series, we may want to try out the so called Integrated processes, and conditionally heteroskedastic processes.
 
### 6.1. Diagnostic Tests
 
First we will test whether the original time series after extracting the seasonal component has the properties of a non-stationary process. The most common approach involves testing for unit-roots of the process' characteristic polynomial. We begin by testing the residuals after filtering seasonal components via the Dickey-Fuller (DF) test.
 
#### 6.1.1 Dickey-Fuller Test (DF)
 
The null hypothesis of the DF test states: $x_t = x_{t-1} + e_t$, that is, the time series is a realization of a random walk process. It is carried out by setting $\Delta x_t = x_t - x_{t-1} = \rho \ x_{t-1} + e_t$ where $e_t$ is an i.i.d process. It suffices to show that the $\rho$ parameter of such regression equals zero. In general the regression may assume $\Delta x_t = \mu + \delta \ t + \rho \ x_{t-1} + e_t$ with additional parameters $\mu$ (mean value) and $\delta$ (deterministic drift).

```{r}
x <- temp_test$Tres
x <- as.numeric(x)
dx <- diff(x) # first differences
m <- length(dx)

xt1 <- x[1:m]
tt <- 1:m
reg <- lm(dx ~ xt1 + 1 + tt)

( reg.sum <- summary(reg) )
```

And after the regression of the difference time series we proceed to compute the $DF$-statistic from the `rho` parameter

```{r}
rho <- reg.sum$coefficients[2,1]
sigma_rho <- reg.sum$coefficients[2,2]
( DFstat <- rho / sigma_rho )
```

The value has to be compared with the critical values for the `DF`-statistic, whose distribution is estimated via simulations
 for particular sample sizes:

```{r}
critVals <- -c(3.60, 3.50, 3.45, 3.43, 3.42, 3.41) # for alpha = 0.05
sampleSizes <- c(25, 50, 100, 250, 500, 100000) # sample size

( critVal <- approx(sampleSizes, critVals, m, rule=2)$y )

if (DFstat < critVal) {
  message(paste(" DF = ", round(DFstat, digits=4), " < ", critVal ,", rho = ", round(rho, digits=4),
                " ::: Null Hypothesis accepted. Possible stochastic trend."))
} else {
  message(paste(" DF = ", round(DFstat, digits=4), " > ", critVal, ", rho = ", round(rho, digits=4),
                " ::: Null Hypothesis rejected. Stochastic trend insignificant."))
}
```

The presence of a stochastic trend for process $\Delta x_t = \mu + \delta \ t + \rho \ x_{t-1} + e_t$ is accepted on a 5% significance level. This means that the time series may be modelled as an integrated process. We can compare the results with the inbuilt `adf.test` function from `tseries` package:

```{r}
require(tseries)
adf.test(x, k = 0) # k = 0 corresponds to lag = 1
```

#### 6.1.2 Augmented Dickey-Fuller Test (ADF)
 
The same procedure can be generalized for lags $k > 0$ as well. For that we can just use the method from `tseries` package:

```{r}
DFi <- list()
for (i in 1:5) DFi[[i]] <- suppressWarnings(adf.test(temp_test$Tres, k = i)$statistic)
DFi <- data.frame(matrix(unlist(DFi), nrow=length(DFi), byrow=T))
names(DFi) <- c("DF")
DFi
```

The ADF method sets a default lag value to 

```{r}
( k <- trunc((length(x)-1)^(1/3))  )

adf.test(x)
```

#### 6.1.3 Phillips-Perron Test
 
Similarily to the DF test, the PP-test is, again, a unit-root test which relies on regressing $\Delta X \sim \mu + \delta \ t + \rho X_{t - 1}$. While the ADF test adresses the issue of the time series being potentially generated by a process of higher orders by using lags $k = 1, 2, ...$ and expanding the regression equation, PP-test makes a non-parametric correction to the t-statistic.
 
```{r}
pp.test(x)
```

This makes the PP-test robust with respect to unspecified autocorrelation and heteroskedasticity.
 
#### 6.1.4 KPSS Test
 
Published by Kwiatkovski, Philips, Schmidt, and Shin (1992), another test approaches the stationarity-vs-non-stationarity issue from an opposite point of view. Using KPSS we can test for stationarity hypothesis (about a deterministic trend). Contrary to the previously used tests, the presence of a unit root is the alternative. It can be used in cases when unit roots hypothesis cannot be rejected and the time series coult be non-stationary (or we have a sample of insufficient length). The test relies on testing for zero variance of an i.i.d process present in the stochastic trend part in $x_t \sim ST_t + \epsilon_t$, or alternatively with deterministic trend: $x_t \sim ST_t + \delta \ t + \epsilon_t$. 

```{r}
x <- as.vector(x, mode="double")
n <- length(x)
ntrend.reg <- lm(x ~ 1)
summary(ntrend.reg)
e <- residuals(ntrend.reg)
table <- c(0.739, 0.574, 0.463, 0.347) # KPSS table for regression without drift

tablep <- c(0.01, 0.025, 0.05, 0.10)
s <- cumsum(e)
eta <- sum(s^2)/(n^2)
s2 <- sum(e^2)/n

l <- 3 # truncation lag parameter
s2 <- .C("tseries_pp_sum", as.vector(e, mode="double"), as.integer(n), as.integer(l), s2=as.double(s2))$s2
kpssStat <- eta/s2
pval <- approx(table, tablep, kpssStat, rule=2)$y
c(KPSS=kpssStat, pval=pval)
```

As it appears, the stationarity null hypothesis is rejected. With `pvalue > 0.1`. The same result can be obtained using the `kpss.test` function from the `tseries` package:

```{r}
( testNDrift <- kpss.test(x, null="Level") )
( testDrift <- kpss.test(x, null="Trend") )
```

where the second result corresponds to testing the second type of regression $x \sim \delta \ t$ with trend, which shows that we can accept the stationarity hypothesis with p-value 

```{r}
testDrift$p.value
```

By regressing the given trend

```{r}
t <- 1:n
trend.reg <- lm(x ~ t)
summary(trend.reg)
e <- residuals(trend.reg)
table2 <- c(0.216, 0.176, 0.146, 0.119) # KPSS statistic table for regression with drift
s <- cumsum(e)
eta <- sum(s^2)/(n^2)
s2 <- sum(e^2)/n
s2 <- .C("tseries_pp_sum", as.vector(e, mode="double"), as.integer(n), as.integer(l), s2=as.double(s2))$s2
kpssStat <- eta/s2
pval <- approx(table2, tablep, kpssStat, rule=2)$y
c(KPSS=kpssStat, pval=pval)
```

This is possibly due to the cut-off of the test part of the time series, which does not entail its entire 365-day period. The test can also be carried out for larger values of truncation lag parameter:

```{r}
kpss.test(x, null="Level", lshort = F)
kpss.test(x, null="Trend", lshort = F)
```

### 6.2 Identifying Models of Integrated Processes
 
We analyze the time series visually:

```{r integRegressionAcf0, fig.width=10, fig.height=4, echo=FALSE}
par(mfrow=c(1,3))
plot(x=temp_test$time, y=x, main="x_t",xlab="day",ylab="T[°C]",axes=F, type="l")
axis(side=1, at=seq(1, 365, 7))
axis(side=2, at=seq(round(min(x), digits=1), round(max(x), digits=1), by=2))
box()

ACF <- acf(x, lag.max=50, plot=F)
lags <- which(abs(acf(x, lag.max=50, main="ACF")$acf[-1]) > 2/sqrt(n))
lagValues <- numeric()
for(i in 1:length(lags)) {
  lagValues[i] <- ACF$acf[lags[i] + 1]
  segments(lags[i], 0, lags[i], lagValues[i], col= "red", lwd=2)
}
acf(x, lag.max=50, main="PACF", type="partial")
```

The ACF and PACF plots above might suggest which correlation lags remain in the residues after extracting the seasonal component. More precisely:

```{r}
data.frame(cbind(lag=lags, ACF=lagValues))
```

The correlation within the data is the highest for $L=1, ... , 5$ and then for $L=44$ and $L=45$. Aside from that, it seems that the time series does not need any stabilisation or differentiation, hence we will consider the original data denoted as `x`. And taking also the conclusions of the previous tests into account, we can proceed to model the data as integrated processes.
 First, we need to estimate the difference order $d$. For most processes $d = 1$. The remaining parameters can be automatically estimated via a search function, for example:

```{r}
require(forecast)
auto.arima(x, d=1, ic = "bic")
```

However if we do not specify a difference order, the `auto.arima` method 

```{r}
auto.arima(x, ic = "bic")
```

returns the best estimate equivalent to an $ARMA(1, 0)$ model, which is what the Hannan-Rissanen procedure in section 4 returned. This means that the prior ARMA estimation provided better results. The estimation procedures however, do not consider the possibility that we may be dealing with a process with long memory (ARFIMA), thus we will attempt to model the time series `x` as different integrated processes of type ARIMA, SARIMA or alternatively ARFIMA and GARCH models respectively.
 
#### 6.2.1 ARIMA
 
Since we have already searched for ARIMA type models in the search space of the time series, we only need to consider processes of integration order $d > 0$. For comparison we consiter models with $d = 0$ which we analyzed in sections 4 and 5.

```{r}
pmax = 5; qmax = 5; dmax = 2;
models.arima <- list();
for (p in 0:pmax) {
  for (q in 0:qmax) {
    for (d in 0:dmax)  {
      tmp <- tryCatch(arima(x, order=c(p,d,q)), error=function(e) e, warning=function(w) w)
      if (length(tmp$message) == 0) {
        key <- paste(p,",",d,",",q)
        models.arima[[key]] <- c(p=p, d=d, q=q, rss=tmp$sigma2, AIC=tmp$aic, BIC=InfCrit(tmp, type="BIC"))
      }
    }
  }
}

models.arima <- data.frame(matrix(unlist(models.arima), nrow=length(models.arima), byrow=T))
names(models.arima) <- c("p", "d", "q", "RSS", "AIC", "BIC")
head(models.arima)
```

And like we did in section 4, after getting a large set of models of varying quality, we sort the resulting list according to their $BIC$'s:

```{r}
bic <- models.arima[6]
bestArima <- models.arima[order(bic),]
head(bestArima)
```

 And notice how models of integrated processes occupy the first 6 places, even though the best model is a stationary $ARMA(1,0)$. For future use we can just filter the models with $d > 0$:

```{r}
bestArima1 <- bestArima[which(bestArima[2] == 1),]
head(bestArima1)
```

#### 6.2.2 SARIMA
 
Seasonal Auto-Regressive Integrated Moving-Average type models use the fact that for each value $x_t$ the model retains memory from its previous period of length $L$, of potentially $D$-th difference, giving rise to additional parameters
 $(P, D, Q)$ and $L$ to an ARIMA model.
 For example we can model the residual time series as a $SARIMA(1,1,1)\times (1,1,1)[10]$ (with a 10-day period):

```{r}
( mod <- arima(x, order=c(1,1,1), seasonal=list(order=c(1,1,1), period=10)) )
```

and compare the results with an $ARIMA(1,1,1)$ model:

```{r arimavssarima, fig.width=10, fig.height=4}
mod2 <- arima(x, order=c(1,1,1))

plot(x, xlab="day", ylab="[°C]", main="ARIMA vs SARIMA")
lines(x - mod2$residuals, col="blue", lwd=2)
lines(x - mod$residuals, col="red", lwd=2)
legend("topleft", legend=c("ARIMA(1,1,1)","SARIMA(1,1,1)x(1,1,1)[10]"),
       col=c("blue","red"), lty=1, bty="o",lwd=2 , cex=0.8)
```

We notice how the SARIMA model fits the values within the first period almost completely. The model residuals approach zero since the model has no previous period to base its fitted values upon. 
  However, it should be noted that quite possibly, all of the actual seasonality (not including any cycles of less than 365-day length) has
been filtered out. We can still forcibly model the residuals as a SARIMA process. 
  To do so we must search through a much larger parameter space (7-dimensional instead of the 3-dimensional as previously in the case of ARIMA models). This will, of course take a substantial amount of time. After searching through combinations of ARIMA $(p,d,q)$ and SARIMA parameters $(P,D,Q)$ for periods $L = 13, 16, 22$ and $45$ days, we chose the following orders:

```{r}
sarimaParams <- list(
  list(p=1, d=0, q=0, P=0, D=1, Q=1),
  list(p=1, d=0, q=1, P=0, D=1, Q=1),
  list(p=2, d=0, q=0, P=0, D=1, Q=1),
  list(p=0, d=1, q=2, P=0, D=1, Q=1),
  list(p=2, d=1, q=1, P=0, D=1, Q=1)
)

sarimaPeriods <- c(13, 16, 22, 45)
```

which we plug into the `arima` method into `seasonal` parameter to obtain the following SARIMA-type models:

```{r}
models.sarima <- list()
for (L in sarimaPeriods) {
  for (i in 1:length(sarimaParams)) {
    p = sarimaParams[[i]]$p; d = sarimaParams[[i]]$d; q = sarimaParams[[i]]$q;
    P = sarimaParams[[i]]$P; D = sarimaParams[[i]]$D; Q = sarimaParams[[i]]$Q;
    model <- arima(x, order=c(p,d,q), seasonal=list(order=c(P,D,Q), period=L))
    key <- paste("(",p,",",d,",",q,")(",P,",",D,",",Q,")[",L,"]", sep="")
    
    models.sarima[[key]] <- c(p=p, d=d, q=q, P=P, D=D, Q=Q, L=L, rss=model$sigma2, AIC=model$aic, BIC=InfCrit(model, type="BIC"))
  }
}

models.sarima <- data.frame(matrix(unlist(models.sarima), nrow=length(models.sarima), byrow=T))
names(models.sarima) <- c("p", "d", "q", "P", "D", "Q", "L", "RSS", "AIC", "BIC")
head(models.sarima)

bic <- models.sarima[10]
bestSarima <- models.sarima[order(bic),]
head(bestSarima)
```

The SARIMA periods were chosen by searching through a much larger parameter space (with incremental periods 1 to 49). For the purposes of this presentation, we omit the procedure to save computation time. Even though the resulting models have a lower BIC their accuracy should be taken with some skepticism, since there are no naturally occuring cycles in the annual temperature measurements aside from a 365-day period.
 
#### 6.2.3 ARFIMA
 
So far we have restricted ourselves to searching through parameter spaces of smaller lag order values (for the sake of reducing computation time). There is however, an alternative for modelling ARIMA-type processes with long memory. This may assess the fact that the nonzero values of the data's ACF still appear after 30 steps.

```{r}
suppressMessages(pkgTest("fracdiff"))
model.fd <- fracdiff(x, nar=1, nma=0)
model.fd$d

model.fd$log.likelihood

summary(modelf <- forecast::arfima(x))

plot(forecast::forecast(modelf, h=100))

pmax = 5; qmax = 5;
models.arfima <- list()
for (p in 0:pmax) {
  for (q in 0:qmax) {
    tmp <- tryCatch(fracdiff(x, nar=p, nma=q), error=function(e) e, warning=function(w) w)
    if (length(tmp$message) == 0) {
      d <- tmp$d
      key <- paste(p,",",d,",",q)
      models.arfima[[key]] <- c(p=p, d=d, q=q, rss=tmp$sigma, BIC=(log(n) * (p + q + 1) - 2 * tmp$log.likelihood))
    }
  }
}

models.arfima <- data.frame(matrix(unlist(models.arfima), nrow=length(models.arfima), byrow=T))
names(models.arfima) <- c("p", "d", "q", "RSS", "BIC")
head(models.arfima)

bic <- models.arfima[5]
bestArfima <- models.arfima[order(bic),]
head(bestArfima)
```

#### 6.2.4 GARCH 
 
Short for Generalized Auto-Regressive Conditionally Heteroskedastic, this set of models owe their non-stationary properties to non-constant local variance (skedasticity) in time. This model is mainly used 

```{r}
library(lmtest)
( bpres_studentized <- bptest(model.cCos, studentize=T) )
( bpres_nstud <- bptest(model.cCos, studentize=F) )
```

In our case the `bptest` suggests homoskedasticity of the used systematic regression (both studentized and not studentized versions). For non-studentized BP-test homoskedasticity almost gets accepted at just above 5%. This means that our data (now `x`) might not be suitable for an ARCH-type model. Nonetheless we can choose the residuals of a model we deem most fit for `x`:

```{r garchData, fig.width=7, fig.height=4}
par(mfrow=c(1,2))
acf(x, lag=50, main="resid ACF")
acf(x, lag=50, main="resid PACF", type="partial")
```

The residuals `x` of the seasonal model `sCos` seem to be slightly correlated. This correlation is well assessed by ARIMA type models. Nevertheless if we will attempt to model them as a conditionally heteroskedastic model. For a generalized ARCH (GARCH) model we have 2 parameters `p` and `q` to choose. They are analogous to AR and MA terms, except they are also used as dimensions for another regression, namely for $\sigma_t^2$ which is the square series of the varying residuals. 

```{r garchACF, fig.width=10, fig.height=4}
require(tseries)
p <- 1; q <- 0;
summary(modelGarch <- garch(x, order=c(p, q), trace=F))
garch_res <- modelGarch$residuals
garch_res <- na.omit(garch_res)

par(mfrow=c(1,1))
plot(x, xlab="day", ylab="[°C]", main="ARIMA vs GARCH")
lines(x - mod2$residuals, col="cornflowerblue", lwd=2)
lines(x - modelGarch$residuals, col="coral4", lwd=2)
legend("topleft", legend=c("ARIMA(1,1,1)","GARCH(1,0)"),
       col=c("cornflowerblue","coral4"), lty=1, bty="o",lwd=2 , cex=0.8)

par(mfrow=c(1,2))
acf(garch_res, lag=50, main=paste("GARCH(",p,",",q,") res"))
acf(garch_res, lag=50, main=paste("GARCH(",p,",",q,") res"), type="partial")

```

The results suggest that a GARCH fit has no effect on the correlation of residuals `x`. Also considering the fact that the BP test confirmed the data's homoskedasticity, we should not use this model at all.
 
### 6.3 Predictive Properties of the Chosen Models
 
```{r predictComparison, fig.width=11, fig.height=5.5}
X <- model.sCos$residuals

par(mfrow=c(1,1))
plot_models <- vector()

model1_id <- 1;
p = bestArima[model1_id, 1]; d = bestArima[model1_id, 2]; q = bestArima[model1_id, 3];
key <- paste("(",p,",",q,")", sep="")
name <- paste("ARMA", key, sep="")
plot_models[1] <- name

ARMA_predict <- tail(fitted(Arima(X, order=c(p,d,q), model=arima(x, order=c(p,d,q)), xreg=model.sCos)), ne)

model2_id <- 1;
p = bestArima1[model2_id, 1]; d = bestArima1[model2_id, 2]; q = bestArima1[model2_id, 3];
key <- paste("(",p,",",d,",",q,")", sep="")
name <- paste("ARIMA", key, sep="")
plot_models[2] <- name

ARIMA_predict <- tail(fitted(Arima(X, order=c(p,d,q), model=arima(x, order=c(p,d,q)), xreg=model.sCos)), ne)

model3_id <- 2;
p = bestSarima[model3_id, 1]; d = bestSarima[model3_id, 2]; q = bestSarima[model3_id, 3];
P = bestSarima[model3_id, 4]; D = bestSarima[model3_id, 5]; Q = bestSarima[model3_id, 6];
L = bestSarima[model3_id, 7];
key <- paste("(",p,",",d,",",q,")(",P,",",D,",",Q,")[",L,"]", sep="")
name <- paste("SARIMA", key, sep="")
plot_models[3] <- name

SARIMA_predict <- tail(fitted(
  Arima(X, order=c(p,d,q), 
        model=arima(x, order=c(p,d,q), seasonal=list(order=c(P,D,Q), period=L)), 
        seasonal=list(order=c(P,D,Q), period=L)), xreg=model.sCos), ne)

model4_id <- 1;
p = bestArfima[model4_id, 1]; d = bestArfima[model4_id, 2]; q = bestArfima[model4_id, 3];
key <- paste("(",p,",",round(d,2),",",q,")", sep="")
name <- paste("ARFIMA", key, sep="")
plot_models[4] <- name

ARFIMA_predict <- tail(fitted(arfima(X, drange=c(0, d), model=fracdiff(x, nar=p, nma=q)), xreg=model.sCos), ne)

x_test <- x + temp_test$sCos
ARMA_predict <- as.numeric(ARMA_predict + temp_eval$sCos)
ARIMA_predict <- as.numeric(ARIMA_predict + temp_eval$sCos)
SARIMA_predict <- as.numeric(SARIMA_predict + temp_eval$sCos)
ARFIMA_predict <- as.numeric(ARFIMA_predict + temp_eval$sCos)

colors <- c(hsv((9.5 - 2) / 8), hsv((9.5 - 3) / 8), hsv((9.5 - 5) / 8), hsv(h=(9.5 - 7) / 8, v=0.7))

plot(x_test, type="l", 
     xlim=c(floor(0.9*length(x)), length(X)), ylim=c(3.5, 17),
     main="1-step predictions of chosen models", xlab="day", ylab="[°C]")
lines(x=nt:(nt + 1), y=temp$T[nt:(nt + 1)])
lines(x=(nt + 1):(nt + ne), y=ARMA_predict, lwd=2, col=colors[1], type="l")
lines(x=(nt + 1):(nt + ne), y=ARIMA_predict, lwd=2, col=colors[2], type="l")
lines(x=(nt + 1):(nt + ne), y=SARIMA_predict, lwd=2, col=colors[3], type="l")
lines(x=(nt + 1):(nt + ne), y=ARFIMA_predict, lwd=2, col=colors[4], type="l")

lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="black", type="p", pch=1)
legend("bottomleft", legend=c(plot_models,"data"),
       col=c(colors, "black"), lty=c(1, 1, 1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, NA, NA, 1))
```

We also consider the predictive properties of the chosen set of models. We will be comparing the top 6 models from each type  (ARIMA, SARIMA, ARFIMA), more specifically according to the root mean square error (RMSE) of 1-step predictions
 
#### 6.3.1 ARIMA

```{r arimaPreds, fig.width=12, fig.height=5}
errors <- list()
xp_resids <- list()

library(forecast)
par(mfrow=c(1,2))
for (i in 1:6) {
  p = bestArima1[i,1]; d = bestArima1[i,2]; q = bestArima1[i,3];
  key <- paste("(",p,",",d,",",q,")", sep="")
  name <- paste("ARIMA", key, sep="")

  model_estim <- arima(x, order=c(p,d,q))
  model <- Arima(X, order=c(p,d,q), model=model_estim, xreg=model.sCos)

  xp <- tail(fitted(model), ne)
  fc <- forecast(model_estim, h=ne)
  
  xp_res <- temp_eval$Tres - xp
  rmse <- sqrt(mean(xp_res^2))
  rmse_h <- sqrt(mean((fc$mean - xp)^2))
  errors[[name]] <- c(model=name, RMSE=round(rmse, 5), RMSE_ne=round(rmse_h, 5), BIC=round(bestArima[i, 6], 3))
  xp_resids[[name]] <- xp_res

  fc$mean <- fc$mean + temp_eval$sCos
  fc$lower <- fc$lower + temp_eval$sCos
  fc$upper <- fc$upper + temp_eval$sCos
  fc$x <- fc$x + temp_test$sCos
  xp <- xp + temp_eval$sCos
  plot(fc, lwd=1, xlab="day", ylab="[°C]", main=name, xlim=c(floor(0.6*length(x)), length(X)))
  mtext(paste(" RMSE(1) = ", round(rmse, digits=3), " ,  RMSE(", ne, ") = ", round(rmse_h, digits=3), sep=""))
  lines(x=(nt + 1):(nt + ne), y=xp, lwd=2, col="green4", type="l")
  lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="red", type="p", pch=20)
  legend("bottomleft", legend=c(paste("prediction (", ne,"-step)", sep=""), "1-step prediction","data"),
         col=c("blue", "green4","red"), lty=c(1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, 20))
}
```

#### 6.3.2 SARIMA

```{r sarimaPreds, fig.width=12, fig.height=5}
par(mfrow=c(1,2))
for (i in 1:6) {
  p = bestSarima[i, 1]; d = bestSarima[i, 2]; q = bestSarima[i, 3];
  P = bestSarima[i, 4]; D = bestSarima[i, 5]; Q = bestSarima[i, 6];
  L = bestSarima[i, 7];
  key <- paste("(",p,",",d,",",q,")(",P,",",D,",",Q,")[",L,"]", sep="")
  name <- paste(ifelse(P + D + Q > 0, "S", ""),"ARIMA", key, sep="")
  
  model_estim <- arima(x, order=c(p,d,q), seasonal=list(order=c(P,D,Q), period=L))
  tmp <- tryCatch(
    Arima(X, order=c(p,d,q), seasonal=list(order=c(P,D,Q), period=L), xreg=model.matrix(model.sCos))
    , error=function(e) e, warning=function(w) w)
  if (length(tmp$message) == 0) {
    model <- tmp
  } else {
    model <- Arima(X, order=c(p,d,q), seasonal=list(order=c(P,D,Q), period=L)) # for some models using xreg results in a singular reg matrix
  }
  
  xp <- tail(fitted(model), ne)
  fc <- forecast(model_estim, h=ne)
  
  xp_res <- temp_eval$Tres - xp
  rmse <- sqrt(mean(xp_res^2))
  rmse_h <- sqrt(mean((fc$mean - xp)^2))
  errors[[name]] <- c(model=name, RMSE=round(rmse, 5), RMSE_ne=round(rmse_h, 5), BIC=round(bestSarima[i, 10], 3))
  xp_resids[[name]] <- xp_res
  
  fc$mean <- fc$mean + temp_eval$sCos
  fc$lower <- fc$lower + temp_eval$sCos
  fc$upper <- fc$upper + temp_eval$sCos
  fc$x <- fc$x + temp_test$sCos
  xp <- xp + temp_eval$sCos
  plot(fc, lwd=1, xlab="day", ylab="[°C]", main=name, xlim=c(floor(0.6*length(x)), length(X)))
  mtext(paste(" RMSE(1) = ", round(rmse, digits=3), " ,  RMSE(", ne, ") = ", round(rmse_h, digits=3), sep=""))
  lines(x=(nt + 1):(nt + ne), y=xp, lwd=2, col="green4", type="l")
  lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="red", type="p", pch=20)
  legend("bottomleft", legend=c(paste("prediction (", ne,"-step)", sep=""), "1-step prediction","data"),
         col=c("blue", "green4","red"), lty=c(1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, 20))
}
```

Here we notice that the stabilisation effect in prediction values is not present in SARIMA `ne`-step predictions (length of the eval. part). This is caused by the periodic correlation within the series. Each step is correlated not only up to `p` steps in true history, `q` steps in noise history, with `d`-th unit root order, but also the same combination of correlations `L` steps into the past. This is the reason why the fitted values seem to copy the time series in its first period, and then follow with the "memory" of this preceding section. Predicting an arbitrary number of steps will result in a repeating periodic pattern which does not decay, since it regresses against its values so far back.
 
In the prediction plots we notice that some models, particularly those with higher MA degree, differ quite substantially in the multiple-step prediction mean value compared to single-step predictions. This is most most noticeable in model:

```{r}
diffRMSE <- lapply(errors, function(e) c(model=e[1], dRMSE=abs(as.numeric(e[2]) - as.numeric(e[3]))))
diffRMSE <- data.frame(matrix(unlist(diffRMSE), nrow=length(diffRMSE), byrow=T))
names(diffRMSE) <- c("model", "dRMSE")
dRMSE <- diffRMSE[2]
( dRMSEmax <- diffRMSE[order(dRMSE, decreasing=T),][1,] )
```

#### 6.3.3 ARFIMA

```{r arfimaPreds, fig.width=12, fig.height=5}
library(fracdiff)

par(mfrow=c(1,2))
for (i in 1:6) {
  p = bestArfima[i, 1]; d = bestArfima[i, 2]; q = bestArfima[i, 3];
  key <- paste("(",p,",",round(d,2),",",q,")", sep="")
  name <- paste("ARFIMA", key, sep="")

  model_estim <- fracdiff(x, nar=p, nma=q)
  model <- arfima(X, drange=c(0, d), model=model_estim, xreg=model.sCos)
  
  xp <- tail(fitted(model), ne)
  fc <- forecast(model_estim, h=ne)
  
  xp_res <- temp_eval$Tres - xp
  rmse <- sqrt(mean(xp_res^2))
  rmse_h <- sqrt(mean((fc$mean - xp)^2))
  errors[[name]] <- c(model=name, RMSE=round(rmse, 5), RMSE_ne=round(rmse_h, 5), BIC=round(bestArfima[i, 5], 3))
  xp_resids[[name]] <- xp_res
  
  fc$mean <- fc$mean + temp_eval$sCos
  fc$lower <- fc$lower + temp_eval$sCos
  fc$upper <- fc$upper + temp_eval$sCos
  fc$x <- fc$x + temp_test$sCos
  xp <- xp + temp_eval$sCos
  plot(fc, lwd=1, xlab="day", ylab="[°C]", main=name, xlim=c(floor(0.6*length(x)), length(X)))
  mtext(paste(" RMSE(1) = ", round(rmse, digits=3), " ,  RMSE(", ne, ") = ", round(rmse_h, digits=3), sep=""))
  lines(x=(nt + 1):(nt + ne), y=xp, lwd=2, col="green4", type="l")
  lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="red", type="p", pch=20)
  legend("bottomleft", legend=c(paste("prediction (", ne,"-step)", sep=""), "1-step prediction","data"),
         col=c("blue", "green4","red"), lty=c(1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, 20))
}
```


#### 6.3.4 Comparison of Predictive Properties

So far, we have chosen 6 best models from each category (ARIMA, SARIMA, ARFIMA) of integrated processes. These 18 possibilities (23 if we include ARMA models from section 4) have to be sorted according to their predictive properties, namely errors and DM test. Since an $18 \times 18$ (or $23 \times 23$ if we also include ARMA models) Diebold-Mariano matrix seems too large for practical purposes, we will choose 5 models from all  categories with the lowest `RMSE` of single-step predictions.

```{r}
# we can also include errors from top 5 ARMA list

for (i in 1:5) {
  name <- paste("ARMA(", orders[[i]],")", sep="")
  errors[[name]] <- c(model=name, RMSE=round(sqrt(merrors[[i]]), 5), RMSE_ne="-", BIC=round(as.numeric(bestArma[i, 1]), 3))
  xp_resids[[name]] <- temp_eval$Tres - eval.predictions[[i]]
}

xp_resids <- matrix(unlist(xp_resids), nrow=length(xp_resids), byrow=T)
errors <- data.frame(matrix(unlist(errors), nrow=length(errors), byrow=T))
names(errors) <- c("model", "RMSE(1)", paste("RMSE(", ne, ")", sep=""), "BIC")

# sort by RMSE(1)
rmse1 <- errors[2]
lowestRMSE1_resids <- xp_resids[order(rmse1),]
( lowestRMSE1 <- errors[order(rmse1),] )
```

And the 5 models with the lowest RMSE are:

```{r}
head(lowestRMSE1, 5)
```

And we can apply the DM test on first 6 models:

```{r}
npred_models <- 6
DieboldMarianoMatrix <- matrix(0, ncol=npred_models, nrow=npred_models)
pvalueMatrix <- matrix(1, ncol=npred_models, nrow=npred_models)
for(i in 1:npred_models) {
  for(j in 1:npred_models) {
    if(i==j) next
    res_i <- lowestRMSE1_resids[i,]; res_j <- lowestRMSE1_resids[j,];
    test <- forecast::dm.test(res_i, res_j)
    DieboldMarianoMatrix[i,j] <- (test$p.value < alpha) * sign(test$statistic) 
    pvalueMatrix[i,j] <- round(test$p.value, 3)
  }
}
DieboldMarianoMatrix

pvalueMatrix
```

We observe that there are no significant differences between models. Perhaps the 5th and 6th model seem to have most significantly different predictive abilities from the very first. However, if we take into account that none of the SARIMA models we tested have a period grounded in measureable reality, we clearly see why there are no significant differences among the models, as sorted by their `RMSE` errors in ascending order. Significant differences appear only as we increase the dimension of the `DieboldMarianoMatrix` to 23 (all models). With ARIMA and some ARFIMA models having significantly better predictive abilities:

```{r}
better_than <- list()
npred_models <- 23
#DieboldMarianoMatrix <- matrix(0, ncol=npred_models, nrow=npred_models)
#pvalueMatrix <- matrix(1, ncol=npred_models, nrow=npred_models)
for(i in 1:npred_models) {
  for(j in 1:npred_models) {
    if(i==j) next
    res_i <- lowestRMSE1_resids[i,]; res_j <- lowestRMSE1_resids[j,];
    test <- forecast::dm.test(res_i, res_j)
    #DieboldMarianoMatrix[i,j] <- (test$p.value < alpha) * sign(test$statistic) 
    if (test$p.value < alpha && test$statistic > 0) {
      better_than[[as.character(i)]] <- c(model=as.character(lowestRMSE1[i, 1]), betterThan=as.character(lowestRMSE1[j, 1]))
    }
    #pvalueMatrix[i,j] <- round(test$p.value, 3)
  }
}
better_than <- data.frame(matrix(unlist(better_than), nrow=length(better_than), ncol=2, byrow=T))
names(better_than) <- c("model", "better than")
better_than
```

Thus according to Diebold-Mariano test, many of the previously computed ARMA-type models could have better predictive properties than some ARIMA models (and one SARIMA model). 
 
### 6.4 Conclusions
  
As the matter of fact, different types of models of integrated processes capture different properties of the time series. 
While ARIMA models (with $d > 0$) seem to outcompete their ARMA counterparts in both $BIC$ and $RMSE$, the DM-test seems to suggest that some ARMA models provide more accurate predictions than some ARIMA models. With no other model being significantly more accurate (except for $ARIMA(0,1,3)$ being slightly more accurate than $SARIMA(1,0,1)(0,1,1)[45]$) than a SARIMA model, and also considering the sorting according to $BIC$ as well as prediction error $RMSE$, we conclude that the first few SARIMA models are the most accurate. Yet, none of the tested SARIMA models have periods grounded in actual cycles, aside from the annual, 365-day long period, of course. The data could contain some cycles we have not covered with seasonal regression with ordinary sine waves, even perhaps the prevalent 45-day cycle that seems most accurate (according to the model's $BIC$). Thus, from the following list:

```{r, echo=F}
lowestRMSE1[6:23,]
```

sorted according to $RMSE$, we emphasise the validity of $ARIMA(1,1,2)$, $ARIMA(2,1,2)$ and fractionally integrated models $ARFIMA(3,0.01,0)$ and $ARFIMA(0,0.34,0)$, as well as previously described ARMA models, such as $ARMA(1,2)$, especially considering the results shown in the table from section 6.3.4 featuring a comparison of significantly different predictive properties according to the Diebold-Mariano (DM) test.
  We have chosen not to use models of type (G)ARCH since the BP-test of the initial seasonal model confirmed the homoskedasticity of residuals with p-values:

```{r, echo=F}
bpres_studentized$p.value
```
for the studentized version of the test, and

```{r, echo=F}
bpres_nstud$p.value
```

for the non-studentized version.
  Another possible evaluation of all the evaluated models is sorting them according to their $BIC$:
  
```{r, echo=F}
bic <- lowestRMSE1[,4]
lowestBIC <- cbind(lowestRMSE1[order(bic),1:2], lowestRMSE1[order(bic),4])
colnames(lowestBIC) <- c("model", "RMSE(1)", "BIC")
lowestBIC
```

Although dubious in the relevance of its 45-day period, the result featuring a list of top 5 SARIMA models with the lowest $RMSE$ also seems to correlate with the top 5 models with the lowerst $BIC$:
  
```{r, echo=F}
lowestRMSE2 <- cbind(lowestRMSE1[,1:2], lowestRMSE1[,4])
colnames(lowestRMSE2) <- c("model", "RMSE(1)", "BIC")
head(lowestRMSE2, 5)
head(lowestBIC, 5)
```

Thus quantitatively, SARIMA models seem to outcompete the remaining categories (in $RMSE$ and $BIC$) even though mostly ARMA and some ARIMA and ARFIMA models are the reasonable choice.

## 7. Exponential Smoothing Methods

Exponential smoothing techniques were among the first to appear (in 1950's), and were, thanks to being relatively fast, used in various predictions and smoothings of time series. Whereas in MA-type models the finite moving averages contribute to each point using regression extimation of their coefficients, the ES technique uses an exponential window, i.e.: a weighed moving average of all values which decreases exponentially into the past.

### 7.1 Choice of a Smoothing Technique for our Data

Each time series could potentially have three properties which can be smoothed, namely: <br/>

  1.) level (mean value) <br/>
  2.) trend (slope) <br/>
  3.) seasonality (containing weighed averages of each $L$ steps into the past) <br/>

The approach to each property can either be additive ($A$) or multiplicative ($M$). 
We will work with the residuals `temp_test$Tres` with extracted annual period, hence we will not model any seasonality, or trend. We are left with additive and multiplicative models of the exponential smoothing of mean values. The additive smoothing models are governed by the following set of recurrence equations:

$x_t = a_t + \epsilon_t$ (observation eqn') <br/>

$a_t = a_{t-1} + \alpha \epsilon_t$ (state space eqn'),  $\alpha \in [0, 1]$ <br/>

for an additive error, and  <br/>

$x_t = a_{t-1} (1 + \epsilon_t)$ (observation) <br/>

$a_t = a_{t-1} (1 + \alpha \epsilon_t)$ (state),  $\alpha \in [0, 1]$ <br/>

for a multiplicative error. In both cases the error can be expressed as $\epsilon_t = x_t - a_{t - 1} = x_t - \hat{x}_{t|t-1}$. Where 
$\hat{x}_{t|t-1}$ are prediction estimates from the previous step.
 The core idea behind simple exponential smoothing with additive error smoothing can be described using the following algorithm:

```{r}
simpleExpSmooth <- function(x, alpha) {
    n <- length(x)
    x.hat <- c(x[1], numeric(n - 1))
    for (i in 2:n) {  
      x.hat[i] <- alpha * x[i - 1] + (1 - alpha) * x.hat[i - 1]
    }
    
    res <- (x - x.hat)
    k <- 1 # nparam
    RSS <- sum(res^2)
    sigma2 <- RSS / n
    # likelihood of n NID observations
    AIC <- n * log(RSS) + 2 * (k + 1)
    BIC <- n * log(RSS) + (k + 1) * log(n)
    
    result <- list(x=x, x.hat=x.hat, res=res, RSS=RSS, sigma2=sigma2, AIC=AIC, BIC=BIC)
    return(result)
}

alpha0 <- 0.1
alpha1 <- 0.8

es_0 <- simpleExpSmooth(x, alpha0)
es_1 <- simpleExpSmooth(x, alpha1)

xp0 <- es_0$x.hat
xp1 <- es_1$x.hat

# 1-step predictions
x.hat0 <- c(xp0[nt], numeric(ne))
x.hat1 <- c(xp1[nt], numeric(ne))

for(t in 2:(ne + 1)) {
  x.hat0[t] <- as.numeric(alpha0 * X[nt + t - 1] + (1 - alpha0) * x.hat0[t - 1])
  x.hat1[t] <- as.numeric(alpha1 * X[nt + t - 1] + (1 - alpha1) * x.hat1[t - 1])
}

```

The model $AIC$ and $BIC$ are computed with respect to the total of two estimated parameters

```{r simpleEtsEstim, fig.width=12, fig.height=5, echo=F}
plot(x=1:length(X), y=X, main="Simple exponential smoothing, ETS(A, N, N)", xlab="day", ylab="[°C]")
mtext(paste(" BIC = ", round(es_0$BIC, 4), ", ", round(es_1$BIC, 4), sep=""))
lines(xp0, lwd=2, col="brown")
lines(xp1, lwd=1, lty=20, col="brown3")
lines(x=nt:length(X), y=x.hat0, lwd=2, col="blue")
lines(x=nt:length(X), y=x.hat1, lwd=1, lty=20, col="blue3")
legend("topleft", legend=c(paste("ETS(A, N, N), alpha = ", alpha0), paste("1-step prediction, alpha=", alpha0),
                           paste("ETS(A, N, N), alpha = ", alpha1), paste("1-step prediction, alpha=", alpha1),"data (residuals)"),
         col=c("brown", "blue", "brown", "blue3", "black"), lty=c(1, 1, 20, 20, NA), lwd=c(2, 2, 1, 1, 1), cex=0.8, pch=c(NA, NA, NA, NA, 1))


plot(x=1:length(X), y=temp$T, main="Simple exponential smoothing, ETS(A, N, N)", xlab="day", ylab="[°C]")
mtext(paste("RMSE = ", round(sqrt( sum( (x.hat0 - X[nt:length(X)])^2) / (ne - 1)), 4), ", ", round(sqrt( sum( (x.hat1 - X[nt:length(X)])^2) / (ne - 1)), 4), sep=""))
lines(xp0 + as.numeric(model.sCos$fitted.values[1:nt]), lwd=2, col="brown")
lines(xp1 + as.numeric(model.sCos$fitted.values[1:nt]), lwd=1, lty=20, col="brown3")
lines(x=nt:length(X), y=x.hat0 + as.numeric(model.sCos$fitted.values[nt:length(X)]), lwd=2, col="blue")
lines(x=nt:length(X), y=x.hat1 + as.numeric(model.sCos$fitted.values[nt:length(X)]), lwd=1, lty=20, col="blue3")
legend("topleft", legend=c(paste("ETS(A, N, N), alpha = ", alpha0), paste("1-step prediction, alpha=", alpha0),
                           paste("ETS(A, N, N), alpha = ", alpha1), paste("1-step prediction, alpha=", alpha1),"data (original)"),
         col=c("brown", "blue", "brown", "blue3", "black"), lty=c(1, 1, 20, 20, NA), lwd=c(2, 2, 1, 1, 1), cex=0.8, pch=c(NA, NA, NA, NA, 1))
```

We observe that with $\alpha \rightarrow 1^-$ the simple ETS model fits the data more precisely, but at the same time does not account for the error terms properly. 
  Models with different smoothing parameters (as well as additive or multiplicative smoothing techniques) need to be examined according to some information criterion. 

```{r etsEstim, fig.width=12, fig.height=5}
require(forecast)
( fc_ets <- ets(x) )

fc_ets$par

fc_ets$x <- fc_ets$x + temp_test$sCos
fc_ets.fc <- predict(fc_ets, h=ne, level=c(80, 95))

fc_ets.fc$mean <- fc_ets.fc$mean + as.numeric(model.sCos$fitted.values[(nt + 1):(nt + ne)])
fc_ets.fc$upper <- fc_ets.fc$upper + as.numeric(model.sCos$fitted.values[(nt + 1):(nt + ne)])
fc_ets.fc$lower <- fc_ets.fc$lower + as.numeric(model.sCos$fitted.values[(nt + 1):(nt + ne)])

fc_ets$states[,1] <- fc_ets$states[,1] + model.sCos$fitted.values[1:(nt + 1)]
plot(fc_ets)

plot(fc_ets.fc)
lines(x=1:(nt + 1), y=fc_ets$states[,1], lwd=2, lty=2, col="red")

```