---
title: "WindACR1"
author: "Martin Cavarga"
date: "5/20/2019"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
pkgTest <- function(x) {
  if (!require(x,character.only = TRUE)) {
    install.packages(x,dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}
```

# Methods of Time Series Analysis Applied to Weather Measurements on a Wind Farm 

## Introduction

The chosen source of data is a wind farm located in the North Sea (with coords: 54.01433°NL, 6.587667°EL) denoted as LES (1y 2015-01-01 2015-12-31 cfsr) which comes from website http://www.vortexfdc.com/, specifically designed to provide its
  users with weather data for potential wind farm sites. This specific dataset has been obtained (for free) from year 2015, and contains a variety of measurements, ranging from wind velocity magnitude, through the wind direction angle, temperature up to dimensionless characteristics like  the Richardson number "Ri" which corresponds to the ratio of buoyancy over shear flow.
In the first 7 parts of this project, I focus on the analysis of a single variable, namely temperature T [°C]. Since the dataset consist of over 47 000 observations of 11 different variables, and thus might be difficult to visualize, I've chosen to include only the average temperature measurements for each day.

![Location of the wind farm](LES.jpg)
 
 ---------------------------------------------------------------------------------------

## 1. Visualization and Basic Stats

We begin by extracting the data from a downloaded file

```{r}
vortex_dat <- read.table("vortex_TS.txt", header = T, skip = 3, 
                          colClasses = "character")
```

The dataset contains 47808 values, making it too large for our methods. Thus we will choose daily values, namely the average daily temperature. To get the daily data we need to `aggregate` values with the same date:

```{r}
suppressMessages(pkgTest("dplyr"))
names(vortex_dat)[8] <- c("temperature")

vortex_dat <- tidyr::separate(data = vortex_dat, col = YYYYMMDD, 
                               into = c("year","month","day"), sep=c(4,6))
vortex_dat <- tidyr::separate(data = vortex_dat, col = HHMM, 
                               into = c("hour","minute"), sep=2)

vortex_dat <- tidyr::unite(vortex_dat, col=time, year, month, day, hour, minute, sep="-", remove=T)
vortex_dat$time <- as.Date(vortex_dat$time)

temp <- aggregate(as.numeric(vortex_dat$temperature), by=list(vortex_dat$time), mean)
names(temp) <- c("day", "T")

head(temp)

```

Now we can visualize the data as followed:

```{r basicDataPlot1, fig.width=9, fig.height=6}
suppressMessages(pkgTest("rmarkdown"))
suppressMessages(pkgTest("knitr"))

temp$time <- 1:nrow(temp)

par(mfrow=c(1,1))
plot(x=temp$time, y=temp$T, type="p",xlab="day", ylab="T[°C]",main="Daily temperature",axes=F)
lines(x=temp$time, y=temp$T,col="black")
axis(side=1, at=seq(1, 365, 7))
axis(side=2, at=seq(round(min(temp$T), 1), round(max(temp$T), 1), by=2))
box()
```

 Now we can clearly see the periodic character of the time series which corresponds to the 
 seasonal changes in this latitude. For more details about the particular measurements we might
 calculate the basic characteristics of the set of values:

```{r}
stat <- summary(temp$T)
stddev <- sd(temp$T, na.rm=TRUE)
v <- as.numeric(c(stat[1],stat[6],stat[4],stat[3],stddev))
names(v) <- c("Min.","Max.","Mean","Median","Std.Dev.")
v
```

 We see that the minimum temperature does not drop far below zero even during the winter, 
 possibly due to warm currents in the northern Atlantic Ocean.
 Aside from local fluctuations, the same pattern can be expected to appear in the next 
 year, and in the year after that. Perhaps after several decades we might see a change in the 
 annual average temperature due to global warming, for instance.
 
 -----------------------------------------------------------------------------------------
   
## 2 Time Series Decomposition: Trend & Seasonal Components ##

 Now we proceed to make the first step in the analysis of the extracted temperature data. 
 It is more or less clear that we will not be able to observe any linear or exponential trends
 on such small dataset. 
 We can verify the absence of a trend by testing the series with the Mann-Kendall rank test:

```{r}
suppressMessages(pkgTest("randtests"))
randtests::rank.test(temp$T)
```

 Which suggests that there is no significant trend in the sample as a whole. We will return to the test in 
 section 3. 

 The only clearly visible systematic components are the periodic seasonal
 changes due to Earth's tilt with respect to the Ecliptic plane. 
 Clearly, the most visible cycle will repeat with a period of 365 days. Other fractions of this cycle 
 might be present as well. The remaining cycles can be observed by examining the ACF (Autocorrelation Function)
 which can be plotted using just the `acf` command:

```{r basicACFPlot2, fig.width=9, fig.height=4}
par(mfrow=c(1,2))
acf(temp$T, lag.max=365, main="Daily temp ACF")
acf(temp$T, lag.max=365, type="partial",main="Daily temp PACF")
```

 The second plot shows a partial autocorrelation function which also accounts for lags 
 shorter than the given lag $k$, that is: $k-1$ , $k-2$, ... , $2$, $1$.
 With common sense, one deduces that the data already has a 365-day cycle, but from the
 correlogram we see that other smaller cycles are present in the time series as well. 
 Hence we model the cycles with the following periods

```{r}
n <- length(temp$T)
seasons <- c(365, 365/2, 365/4, 365/12)
names(seasons) <- c("S1.","S2.","S3","S4")
seasons
```

 We may assume that there will be a month-long cycles which correspond to the rotation of the Moon, rather
 than the calendar we use:

```{r monthlyPlot, fig.width=10, fig.height=3}
months = c(
  "January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"
)
mdays = c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)
from = 1; to = 0;

par(mfrow=c(1,2))
for (month in 1:12) {
  to = ifelse(month == 12, n, to + mdays[month])
  seg = from:to
  plot(x=1:length(seg), y=temp$T[seg], type="l",
       main=paste("Avg. Daily Temp. in ", months[month]), sub=paste("from day", from, " to ", to), xlab="[day]", ylab="T[°C]", lwd=2, lty=1.5)
  from = to + 1
}
```

The monthly data do not seem to have a repeating pattern. Hence we will only consider the annual 365-day period.
 
From now on we will separate our time series into a test part and an evaluation part. 
All regression analysis will be performed on the test part. Predictions and their quality will
be examined on the evaluation part. Generally the test part is taken as the first 80% of the original
time series:

```{r}
nt <- floor(n * 8 / 10)
temp_test <- list()
temp_test$T <- temp$T[1:nt]
temp_test$time <- temp$time[1:nt]

temp_eval <- list()
temp_eval$T <- temp$T[(nt + 1):n]
temp_eval$time <- temp$time[(nt + 1):n]

par(mfrow=c(1,1))
plot(x=temp$time, y=temp$T, type="p", main="Test and Evaluaion Parts", xlab="time", ylab="T[°C]")
lines(x=temp_test$time, y=temp_test$T, lwd=1.5, col="blue")
lines(x=temp_eval$time, y=temp_eval$T, lwd=1.5, col="green")
legend("topleft", legend=c("test","eval"),
       col=c("blue","green"), lty=1,lwd=2 , cex=0.8)
```

Now we will take the chosen season and regress them against the test part:

```{r}
model.sCos <- lm(T ~ cos(2*pi*temp$time/seasons[1]) + sin(2*pi*temp$time/seasons[1])
                   , temp)
summary(model.sCos)
```

The summary of the regression model shows that amplitudes $C_1 = -4.2990$, $C_2 = -3.5194$, and the intercept: $C_0 = 9.3188$ are significant.
And now we can plot the residues after extracting the seasonal components

```{r residPlot1, fig.width=10, fig.height=4}
temp_test$sCos <- model.sCos$fitted.values[1:nt]
temp_eval$sCos <- model.sCos$fitted.values[(nt + 1):n] # fitted values for later use

temp_test$Tres <- model.sCos$residuals[1:nt]
temp_eval$Tres <- model.sCos$residuals[(nt + 1):n] # residuals for later use

par(mfrow=c(1,2))
plot(T ~ temp_test$time, temp_test,
     main="Fitted Annual Temperature Model",xlab="day",ylab="T[°C]",axes=F)
lines(temp_test$time, temp_test$sCos, col="blue", lwd=2)
axis(side=1, at=seq(1, n, 7))
axis(side=2, at=seq(round(min(temp_test$T),1), round(max(temp_test$T),1), by=2))
lines(temp_test$time, temp_test$sCos, col="blue", lwd=2)
box()

plot(x=temp_test$time, y=temp_test$Tres,
     main="Residuals",xlab="day",ylab="res(T)[°C]",axes=F,type="l")
axis(side=1, at=seq(1, n, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()
```

We can see that the residuals after extracting the seasonal components still have not 
lost some of their periodic behavior.
    
  The resulting regression equation takes the form: <br/>
  
  $\hat{x}_t = \hat{C}_0 + \hat{C}_1 \cos{(2 \pi t / 365)} + \hat{C}_2 \sin{(2 \pi t / 365)} + \hat{\varepsilon}_t$ <br/>
  
  $\hat{x}_t = 9.3188 - 4.2990 \cos{(2 \pi t / 365)} - 3.5194 \sin{(2 \pi t / 365)} + \hat{\varepsilon}_t$ <br/>

The residuals $\varepsilon_t$ with standard error $\sigma_{\hat{\varepsilon_t}} = 2.072$ may or may not be entirely random. We will have to determine this in the following sections.

  We can also include the standard deviations into the coefficients: <br/>
  
  $\hat{x}_t = (9.3188 \pm 0.1116) + (-4.2990 \pm 0.1618) \cos{(2 \pi t / 365)} + (-3.5194 \pm 0.1536) \sin{(2 \pi t / 365)}$ <br/>

Taking a look at the residual ACF and PACF we see 
that non-zero correlation extends from lag of approximately 50 days.

```{r ACFPlot2, fig.width=9, fig.height=4}
par(mfrow=c(1,2))
acf(temp_test$Tres, lag.max=365, main="Residual ACF")
acf(temp_test$Tres, lag.max=365, main="Residual PACF", type="partial")
```

When examining the residual ACF we notice remaining significant correlation for multiple lag
values. This is possible due to remaining cyclical components which will be found as significant
frequencies after Fourier analysis in the next section.

 ---------------------------------------------------------------------------------------
             
## 3 Time Series Decomposition: Cyclical Components & Randomness Tests on Residuals
### 3.1 Cyclical Components and Fourier Analysis on Time Series

Now we continue with another step in the decomposition of the original time series.
We separate the remaining oscillations via discrete Fourier analysis. A continuous spectrum
is generated from the ACF using the following function:

```{r SpecPlot2, fig.width=9, fig.height=4}
SpecDens <- Vectorize(  
  FUN = function(omega, acov=NULL, data=NULL) {
    if(is.null(acov)) {
      if(is.null(data)) {
        stop("Please provide either vector of autocovariance function values or time series data.")
      }
      else acov <- acf(data, type="covariance", plot=FALSE)
    }
    k <- seq(to=length(acov)-1)
    ( acov[1] + 2*sum(acov[-1]*cos(k*omega)) ) / (2*pi)
  },
  vectorize.args = "omega"
)


par(mfrow=c(1,2))
temp_test.ACF <- as.numeric( acf(temp_test$Tres, type="covariance", plot=FALSE)$acf )
plot(function(x) SpecDens(x, acov=temp_test.ACF), from=2*pi/(nt),
     to = 2 * pi / 4, xlab="frequency", ylab="spectral density")
plot(function(x) SpecDens(2*pi / x, acov=temp_test.ACF), from=4, 
     to = nt, xlab="period", ylab="spectral density")
```

The figure on the right provides a better image of the (continuous) distribution of individual
components with periods of oscillation. The hidden frequencies are obtained using discrete
Fourier transform (more specifically Fast Fourier Transform):

```{r SpecDens,fig.width=11, fig.height=3.5}
temp_test.FFT <- abs(fft(temp_test$Tres)) # Fourier transform
omega <- 2 * pi * seq(0, nt / 2 - 1) / nt
period <- 2 * pi / omega
par(mfrow=c(1,1))
plot(period, temp_test.FFT[seq_along(period)], main="Residual Spectral Density", 
     ylab="density", xlab="period (days)", type="h", log="x")
```

which we can then sort by density, and display the ones with the most weight:

```{r}
spectrum <- data.frame(f=temp_test.FFT[seq_along(omega)], omega=omega, period=period)
spectrum <- spectrum[order(spectrum$f, decreasing = TRUE), ]
head(spectrum, 10)
```

One reliable way to find the most significant frequencies is by using Fischer's Periodicity Test:

```{r}
signif <- cbind(spectrum[0,], 
                  data.frame(test.stat = numeric(0), crit.val = numeric(0)))
spec <- spectrum
repeat {
  test <- data.frame(test.stat = spec$f[1]/sum(spec$f),
                     crit.val = 1 - (0.05/nrow(spec))^(1/(nrow(spec)-1)))
  if(test$test.stat > test$crit.val) {
    signif <- rbind(signif, cbind(spec[1,],test));
    spec <- spec[-1,]
  } else break
}
rm(spec, test)
signif
```

As it appears, Fischer's Test yields no significant frequency, but since we observe a sequence of significant frequencies that correspond with the ACF shown earlier. Nonetheless, we assume that, say, the first 10 might contribute to the oscillatory behavior of the time series. Thus we take:

```{r}
( signif <- head(spectrum, 10) )
```

which we can then determine more precisely (by finding their local maxima) using the continuous spectral density:

```{r}
newsignif <- sapply(
  signif$omega,
  function(x) optimize(SpecDens, 
                       interval = x + c(1,-1) * pi / nt, 
                       acov = temp_test.ACF, 
                       maximum = T  )$maximum)
( newSignifs <- cbind(signif, data.frame(omega_sm = newsignif, 
                                       period_sm = 2 * pi / newsignif)) )
```

We can try to complete the regression using the first 3 periods with their fractions, but as it turns out, only the first one with its half is significant.

```{r}
( periods <- head(newSignifs, 3)$period_sm )

model.cCos <- lm(Tres ~ cos(2*pi*temp_test$time/periods[1]) + sin(2*pi*temp_test$time/periods[1]) +
                     cos(2*pi*temp_test$time/periods[1]*2) + sin(2*pi*temp_test$time/periods[1]*2) #+
                   #cos(2*pi*temp_test$time/periods[2]) + sin(2*pi*temp_test$time/periods[2]) +
                   #cos(2*pi*temp_test$time/periods[3]) + sin(2*pi*temp_test$time/periods[3])
                   ,
                 temp_test)
summary(model.cCos)
```

The cyclical components defined by first two periods seem to be significant, but we cannot be sure if they truly
belong into the systematic model. For that reason we will save the residues of this model so that we can compare its
predictive properties with only `sCos` in section 5.

```{r cyclicalRegress,fig.width=10, fig.height=4}
temp_test$cCos <- model.cCos$fitted.values


par(mfrow=c(1,1))
plot(x=temp_test$time, y=temp_test$Tres,
     main="Temperature (Residues)",xlab="day",ylab="T[°C]",axes=F)
axis(side=1, at=seq(1, nt, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
lines(x=temp_test$time, y=temp_test$cCos, col="blue", lwd=2)
box()
```

We can combine the seasonal and cyclical components into a single model:

```{r finalRegress,fig.width=10, fig.height=5}
model.SCcos <- lm(T ~ cos(2*pi*temp$time/seasons[1]) + sin(2*pi*temp$time/seasons[1]) +
                        cos(2*pi*temp$time/periods[1]) + sin(2*pi*temp$time/periods[1]) +
                        cos(2*pi*temp$time/periods[1]*2) + sin(2*pi*temp$time/periods[1]*2),
                      temp)
summary(model.SCcos)

temp_test$SCcos <- model.SCcos$fitted.values[1:nt]
temp_eval$SCcos <- model.SCcos$fitted.values[(nt + 1):n]


par(mfrow=c(1,1))
plot(T ~ temp_test$time, temp_test,
     main="Modeling Seasonal and Cyclical Components",xlab="day",ylab="T[°C]",axes=F)
lines(temp_test$time, temp_test$sCos, col="red", lwd=2)
lines(temp_test$time, temp_test$SCcos, col="blue", lwd=2)
axis(side=1, at=seq(1, nt, 7))
axis(side=2, at=seq(round(min(temp_test$T), digits=1), round(max(temp_test$T), digits=1), by=2))
box()
legend("topleft", legend=c("seasonal","seasonal + Fourier"),
       col=c("red","blue"), lty=1,lwd=2 , cex=0.8)
```

```{r finalResidues,fig.width=10, fig.height=3.5}
#length(temp_test$Tres)
#length(model.sCos$residuals)

temp_test$Tres <- model.sCos$residuals[1:nt]
temp_test$Tres2 <- model.SCcos$residuals[1:nt] # for future use

temp_eval$Tres2 <- model.SCcos$residuals[(nt + 1):n]


plot(x=temp_test$time, y=temp_test$Tres, type="l",
     main="Residuals After Seasonal",xlab="day",ylab="T[°C]",axes=F)
axis(side=1, at=seq(1, nt, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()
plot(x=temp_test$time, y=temp_test$Tres2, type="l",
     main="Residuals After Seasonal & Cyclical",xlab="day",ylab="T[°C]",axes=F)
axis(side=1, at=seq(1, nt, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()
```

 ---------------------------------------------------------------------------------------

### 3.2 Radomness Tests on Residuals After Extracting Systematic Components

Even though the original temperature time series has no trend, the residuals of the resultant time series with extracted both seasonal (and cyclical) components, that is: the systematic components might still contain some cyclical components. To verify that the residuals are a trajectory of a (stationary) stochastic process we use the, so called, "Randomness tests":
 
 1.) Durbin-Watson autocorrelation test
 
 2.) Zero ACF test
 
 3.) Signed Rank Test 
 
 4.) Spearman Rho Test
 
 5.) Turning Point Test 
 
 6.) Median Test 
 
 
We begin by examining the residues after extracting all the systematic components in `model.sCos`, as well as their ACF:

```{r resPlot1, fig.width=10, fig.height=5}
par(mfrow=c(1,2))
plot(x=temp_test$time, y=temp_test$Tres,
     main="Temperature",xlab="day",ylab="T[°C]",axes=F, type="l")
axis(side=1, at=seq(1, 356, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()

acf(temp_test$Tres, lag.max=365, main="Residual ACF")
```

As we can clearly see, the residues still have a hidden oscillatory component with a period of around 30 days. Hence we move ahead to test their randomness via the given tests.
 
#### 3.2.1 Durbin-Watson Test

The null hypothesis of the D-W test states that the residues of a time series after a least squares regression are uncorrelated, which is put against the alternative hypothesis: that the residuals follow a 1st order autoregressive (AR) process (see section 5.). 

We can either use the inbuilt function in the `car` package:

```{r}
suppressMessages(pkgTest("car"))
car::durbinWatsonTest(temp_test$Tres)
```

or write our own function:

```{r}
# only unique values in the time series will be considered
values1 <- as.numeric(temp_test$Tres)
values2 <- unique(values1)

nv <- length(values2)
den <- sum(values2 ^ 2)
( DW <- (sum((values2[2:nv] - values2[1:(nv - 1)])^2))/den )
```

The resulting DW statistic is then compared with the D-W critical values for a sample of given size and a number `k` of the terms of linear regression (intercept included), in our case `k = 2` and `size = 277`, hence in a 5% confidence interval:

```{r}
# dL         dU
# 1.79306  1.80792 (n = 270)
# 1.79690  1.81123 (n = 280)

dL <- 0.5 * (1.79306 + 1.79690)
dU <- 0.5 * (1.80792 + 1.81123)

if(DW <= dL) {
  message("Alternative Hypothesis: Positive Autocorrelation!")
} else if (4 - dL < DW && DW < 4) {
  message("Alternative Hypothesis: Negative Autocorrelation!")
} else if (dU < DW && DW < 4 - dU) {
  message("Null Hypothesis: No Autocorrelation!")
}
```

We can obtain p-values using the model matrix of the `Scos` model, simulating the stochastic process $Y \sim \rho X - \mu$, computing a DW-statistic for each simulation, and counting the number of `DW < simDW`:

```{r}

X <- model.matrix(model.sCos)[1:nt,]

reps = 1000
sig <- var(values2)
mu <- temp_test$sCos
Y <- matrix(rnorm(nv*reps, 0, sig), nv, reps) + matrix(mu, nv, reps)
E <- residuals(lm(Y ~ X - 1))
simDW <- apply(E, 2, function(e) (sum((e[2:length(e)] - e[1:(length(e) - 1)])^2) / sum(e ^ 2)) )

( p_val <- (sum(DW > simDW)) / reps ) # for negative correlation
( p_val <- (sum(DW < simDW)) / reps ) # for positive correlation
( p_val <- 2*min(p_val, 1 - p_val) ) # for two-sided correlation
```

The `rho` parameter appearing in the test is the, so called, autocorrelation coefficient for which $-1 < \rho < 1$ and the null hypothesis translates to verifying that $\rho = 0$ for some confidence interval. 
 
We can model correlation for other lags as well. Aside from the residual vector as an argument, the `durbinWatsonTest`
can process the entire regression model. It also has a `max.lag` argument which tells the function which lag to test for when using a model $Y \sim X - L$ (where $L$ is the lag). Parameter `alternative` also tells the function which type of correlation to test for:

```{r}
car::durbinWatsonTest(model.sCos, max.lag=10, alternative="two-sided")
car::durbinWatsonTest(model.sCos, max.lag=10, alternative="negative")
car::durbinWatsonTest(model.sCos, max.lag=10, alternative="positive")
```

We see that DW tests have zero p-value up to lag 4. This means that we could be dealing with an AR process of order 4 or higher.

#### 3.2.2 Zero ACF Test
The test reduces to just finding all the ACF lags k with $ACF(k) > 2/ \sqrt{n}$ where $n$ is the length of the time series.

```{r acfPlot1, fig.width=11, fig.height=5}
par(mfrow=c(1,1))
ACF <- acf(temp_test$Tres, lag.max=nt, plot=F)
lags <- which(abs(acf(temp_test$Tres, lag.max=nt, main="Residual ACF")$acf[-1]) > 2/sqrt(nt))
lagValues <- numeric()
for(i in 1:length(lags)) {
  lagValues[i] <- ACF$acf[lags[i] + 1]
  segments(lags[i], 0, lags[i], lagValues[i], col= "red", lwd=2)
}
lags
```

The result shows that the residual ACF exceeds the zero-value for the lags shown above.

#### 3.2.3 Signed Rank Test

This is a non-parametric test for the presence of a trend in the residual time series, based on the analysis of differences of sets of three consecutive terms. The null hypothesis states that the resulting statistic is asymptotically normally distributed. using a custom approach:

```{r}
# consecutive values with zero difference have to be omitted
dist_bool <- c(T,as.logical(diff(values2)))
Tt <- c()
for (i in 1:length(dist_bool)) {
  if (dist_bool[i]) Tt <- c(Tt, values2[i])
}

#now the ranking of differences
ranks <- c()
for (i in 2:length(Tt)) {
  if (Tt[i-1] < Tt[i]) {
    ranks[i - 1] <- 1
  } else {
    ranks[i - 1] <- 0
  }
}

rank_sum <- sum(ranks)
rank_mean <- (length(Tt) - 1) / 2
rank_var <- (length(Tt) + 1) / 12
test_stat <- (rank_sum - rank_mean) / sqrt(rank_var)

alpha = 0.05

if (test_stat < qnorm(alpha) || test_stat > qnorm(1 - alpha)) {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Alternative Hypothesis! test_stat is not normally distributed for n-->infty ! Trend detected."))
} else {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Null Hypothesis! test_stat is normally distributed for n-->infty ! No trend detected."))
}
```

And we can compare the results with an inbuilt function from the `randtests` package:

```{r}
require(randtests)
randtests::difference.sign.test(temp_test$Tres)
```

So as it appears, for a 5% confidence interval the residues still contain a trend. Substituting 0.01 for `alpha`,however, gives an alternative hypothesis as a result. Thus the signed-rank test verifies its null hypothesis for $0.1 > \alpha > 0.05$. This is probably caused by cutting off a portion of the data at the end of an annual sample.

#### 3.2.4 Spearman Rho Test

Another non-parametric test for the presence of a trend using concordant/discordant pairs of values:

```{r}
#order the unique values in an increasing order

temp_ordered <- values2[order(values2, decreasing=F)]
q <- numeric()

for (i in 1:length(values2)) {
  q[i] <- match(c(values2[i]), temp_ordered) 
}

# and for the spearman rho coefficient
nv <- length(values2)
sum_term <- numeric()
for (i in 1:length(values2)) {
  sum_term[i] <- (i - q[i])^2 
}

rho <- 1 - (6 / (nv * (nv * nv - 1))) * sum(sum_term)

test_stat <- abs(rho) * sqrt(nv - 1)

if (test_stat <= qnorm(alpha) || test_stat >= qnorm(1 - alpha)) {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Alternative Hypothesis! test_stat is not normally distributed for n-->infty ! Trend detected."))
} else {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Null Hypothesis! test_stat is normally distributed for n-->infty ! No trend detected."))
}
```

using the inbuilt `cor.test`:

```{r}
# our rho:
rho

cor.test(temp_test$time, temp_test$Tres, method="spearman")
```

Clearly both the Signed-Rank, and Spearman Rho tests show that the residual time series is a realisation of independent identically distributed random variables.

#### 3.2.5 Turning Point Test

This and the following test are both tests for the presence of (previously unfiltered) periodic components in the  residual time series. Similarily to the signed rank test, it examines sets of three consecutive terms, except this time looking for so called 'turning points', that is: triplets of values in which the middle value is locally extremal (in that triplet). The presence and periodic regularity of turning points implies oscillatory behavior or the residues.

```{r}
dist_bool <- c(T,as.logical(diff(values2)))

#removing duplicate consecutive values
Tt <- c()
for (i in 1:length(dist_bool)) {
  if (dist_bool[i] == TRUE)  Tt <- c(Tt, values2[i])
}

# marking consecutive triplets with value 0 for upper and lower turning points
# and 1 otherwise

Mt <- c()
for (i in 2:(length(Tt) - 1)) {
  if ( ((Tt[i - 1] < Tt[i]) && (Tt[i] > Tt[i + 1])) || 
       ((Tt[i - 1] > Tt[i]) && (Tt[i] < Tt[i + 1])) ) {
    Mt[i - 1] <- 1
  } else {
    Mt[i - 1] <- 0
  }
}

M_sum <- sum(Mt)
M_mean <- 2 * (length(Tt) - 2) / 3
M_var <- (16 * length(Tt) - 29) / 90

test_stat <- (M_sum - M_mean) / sqrt(M_var)

if (test_stat <= qnorm(alpha) || test_stat >= qnorm(1 - alpha)) {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Alternative Hypothesis! Periodic components detected."))
} else {
  message(paste("test_stat = ",test_stat," alpha = ", alpha,
                " ::: Null Hypothesis! No periodic components detected."))
}
```

The p-value of the test can be evaluated for three different hypotheses:

```{r, echo=T, eval=F}

( p_value <- pnorm(test_stat) ) # positive serial correlation
( p_value <- 2*min(p_value, 1 - p_value)) # (two-sided) non-randomness
( p_value <- 1 - p_value ) # negative serial correlation
```

```{r}
randtests::turning.point.test(temp_test$Tres, alternative="left.sided")
randtests::turning.point.test(temp_test$Tres, alternative="two.sided")
randtests::turning.point.test(temp_test$Tres, alternative="right.sided")
```

#### 3.2.6 Median Test

This is another non-parametric test for the presene of periodic components which essentially divides the sample values into distinct groups each of which corresponds to a group of values that are unilaterally above or below the sample median, meaning that if the $i$-th value and its predecessors lie above the median, and the $(i+1)$-th value lies below, for instance, the $(i+1)$-th value is a member of a new group of values which lie below the sample median.

```{r medianPlot1, fig.width=11, fig.height=5}
U <- temp_test$Tres
temp_med <- median(U)
U_ <- U - temp_med
U_ <- U_[U_ != 0]
M <- U_ > 0
head(as.numeric(M), 10)

c(below=sum(diff(M)<0), above=sum(diff(M)>0))

m <- sum(M)
P <- sum(diff(M)>0) + sum(diff(M)<0) + 1

par(mfrow=c(1,1))
plot(temp_test$Tres ~ temp_test$time, main="Median", ylab="res")
abline(h=temp_med, col="green")
for(i in 1:length(temp_test$Tres)) {
  if (U[i] != temp_med ) {
    if (U[i] > temp_med) {
      segments(temp_test$time[i], temp_med, temp_test$time[i], U[i], col= "red", lwd=1)
    } else {
      segments(temp_test$time[i], temp_med, temp_test$time[i], U[i], col= "blue", lwd=1)
    }
  }
}

( ZStatistic <- (P - (m+1)) / sqrt(m*(m-1)/(2*m-1)) )


if (ZStatistic > qnorm(alpha / 2) && ZStatistic < qnorm(1 - alpha/2)) {
  message(paste("qnorm(",alpha/2,") < Zstat < qnorm(",1 - alpha/2,")"))
  message("", round(qnorm(alpha / 2), digits=2)," < ", round(ZStatistic, digits=2)," < ",round(qnorm(1 - alpha/2), digits=2),"")
  message("Null Hypothesis! Randomness!")
  if (m <= 100) cat("Note: 'above median' group count: m = ", m," <= 100")
} else {
  message(paste("qnorm(",alpha/2,") <= Zstat || Zstat >= qnorm(",1 - alpha/2,")"))
  message("", round(qnorm(alpha / 2), digits=2)," <= ", round(ZStatistic, digits=2)," >= ",round(qnorm(1 - alpha/2), digits=2),"")
  message("Alternative Hypothesis! Non-Randomness ---> Periodicity!")
  if (m <= 100) cat("Note: 'above median' group count : m = ", m," <= 100")
}

( p_value <- pnorm(ZStatistic) ) # positive serial correlation
( p_value <- 2*min(p_value, 1 - p_value)) # (two-sided) non-randomness
( p_value <- 1 - p_value ) # negative serial correlation

randtests::runs.test(U)

# library(signmedian.test) # NOTE: signmedian.test does not do the same thing as the median test above
# signmedian.test(temp_test$Tres, alternative="two.sided")
# signmedian.test(temp_test$Tres, alternative="greater")
# signmedian.test(temp_test$Tres, alternative="less")
```

The median test implies randomness of the residual time series, yet its results are irrelevant for group count $m \leq 100$. The result of the Durbin-Watson Test implies that the residual time series may be a trajectory of an $AR(4)$
(auto-regressive) process, and the residual ACF, of course, shows non-zero correlation for multiple lags. The signed-rank
test "almost" implies that the residues do not have a trend (more specifically, for $\alpha = 0.1$), and the following
Spearman rho test does so as well. The presence of a trend may be caused by cutting off the tail of the annual series.
The Turning Point test, with the Median test which  followed show that the time series also contains unfiltered periodic
components, even though the Median test may be inconclusive since the group count m of groups of 
values above median is less than 100.
 
Therefore, from all the tests carried out in this section, we can conclude the following:
 
 - The residual time series may be a trajectory of an $AR(3)$ (auto-regressive) process
 - There is no trend present in the residual time series
 - And finally the unfiltered periodic components have to be accounted for via other methods


 ---------------------------------------------------------------------------------------------------------------
 
## 4. Introducing ARMA Models and the Hannan-Rissanen Procedure

In general, a stationary (only lag-dependent) stochastic process is determined by p auto-regressive and q moving-average terms. The auto-regressive terms (as by the name) determine the value of the stochastic process at each time from its past values from up to p steps back, and the moving-average terms on the other hand imply dependence on a random process, such as the white noise, for instance, up to q time steps back. 

```{r newRegressionAcf, fig.width=12, fig.height=5}
par(mfrow=c(1,3))
plot(x=temp_test$time, y=temp_test$Tres, main="Residues",xlab="day",ylab="T[°C]",axes=F, type="l")
axis(side=1, at=seq(1, 365, 7))
axis(side=2, at=seq(round(min(temp_test$Tres), digits=1), round(max(temp_test$Tres), digits=1), by=2))
box()

ACF <- acf(temp_test$Tres, lag.max=365, plot=F)
n<-length(temp_test$Tres)
lags <- which(abs(acf(temp_test$Tres, lag.max=365, main="Residual ACF")$acf[-1]) > 2/sqrt(n))
lagValues <- numeric()
for(i in 1:length(lags)) {
  lagValues[i] <- ACF$acf[lags[i] + 1]
  segments(lags[i], 0, lags[i], lagValues[i], col= "red", lwd=2)
}
acf(temp_test$Tres, lag.max=365, main="Residual PACF", type="partial")
```
 
Based on the result of the Durbin-Watson test, the additional oscillations (which can still be seen in red non-zero values of the last residual ACF) should be accounted for by modelling the residues by a suitable $AR(p)$ model (or $ARMA(p,q)$ in general). The process for determining the proper orders $p$ and $q$ of suitable ARMA model candidates is called the Hannan-Rissanen Procedure and it consists of multiple steps.

First, we need to figure out whether the residues need additional transformation, so that they could be properly modelled. For that we test whether:
 (a) the time series has zero mean value
 (b) the time series is stationary

```{r}
mean(temp_test$Tres)
```

So the mean value approximation is close to zero, and the stationarity can be tested using the following test:

```{r}
suppressMessages(pkgTest("tseries"))
adf.test(temp_test$Tres, alternative="explosive") # Augmented Dickey-Fuller
```

According to the Augmented Dickey-Fuller tests the residuals the stationarity hypothesis is rejected. However, since The tests have low statistical power in that they often cannot distinguish between true unit-root processes and near unit-root processes. This is called the "near observation equivalence" problem.

### 4.1. The Hannan-Rissanen Procedure
 
First, we need to set the maximum values for lag parameters based on the significant lags in residual ACF and PACF. It will be easier to do so in a combined plot:

```{r plot03, fig.width=8, fig.height=4}
par(mfrow=c(1,1))
acf(temp_test$Tres, ylab="(P)ACF", lag.max=100, main="Comparison of Residual ACF & PACF")
tmp <- pacf(temp_test$Tres, plot=F, lag.max=100)
points(tmp$lag+0.3, tmp$acf, col="red", type = "h")
legend("topright", legend=c("ACF","PACF"), col = c("black","red"), lty=c(1,1))
```

The partial correlogram (red) shows that the most significant correlations are for lags $L = 1$, and $L = 40$. The second lag, however, does not exceed the zero region nearly as much as $L = 1$. It is then reasonable to assume that the most viable model will be of AR order $p = 1$. Hence:

```{r}
kmax = 90;
```

The Yule-Walker method based on solving regression equations against an increasing basis of AR terms can be done through the inbuilt `ar()` function which automatically finds the model with the lowest AIC (Akaike's Information Criterion). And by plotting the `$aic` parameter we obtain differences $AIC_{min} - AIC_k$ for all models.

```{r}
model <- list()
model$ar.yw <- ar(temp_test$Tres, order.max=kmax)
```

Plotting the differences we notice that the highest drop in $\Delta AIC$ comes after $L=1$ and then a much smaller drop follows after `L=15` and `L=40` after that. We can determine the maximum order $k$ from the lags where this drop in $\Delta AIC$ occurs. An alternative to this approach is determining the AR order $p$ from the residual variances:

```{r plot05, fig.width=9.5, fig.height=4}
par(mfrow=c(1,2))
plot(0:(length(model$ar.yw$aic)-1), xlab="p", model$ar.yw$aic, ylab="dAIC", main="Differences in AIC")
rbind(coef=model$ar.yw$ar, se=sqrt(diag(model$ar.yw$asy.var.coef))) 
tmp <- sapply(1:kmax, function(x) ar(temp_test$Tres, aic=F, order.max=x)$var.pred)
plot(tmp, xlab="p", ylab="sigma^2", main="Residual variances")
```

The plot suggests that the highest order of the AR process should be around `p = 20`. The maximum AR order can also be determined via a recursive Lewinson-Durbin algorithm. For the sake of saving computation time, however, we choose the following maximum order parameters:

```{r, echo=T}    
kmax = 20; pmax = 10; qmax = 10;

```

```{r}

#LongAR method implementation:
LongAR = function(tser, k, p, q) {
  if (!is.ts(tser)) {
    tser = ts(tser)
  }
  
  tt <- data.frame(x=tser)
  tt$z <- ar.ols(tt$x, aic=F, order.max=k)$resid
  tt$z[is.na(tt$z)] <- 0
  
  # suppressMessages(library(dynlm))
  
  if (p > 0 & q > 0)  outmodel <- dynlm(x ~ L(x, 1:p) + L(z, 1:q), tt)
  else if (p > 0 & q == 0) outmodel <- dynlm(x ~ L(x, 1:p), tt)
  else if (q > 0 & p == 0) outmodel <- dynlm(x ~ L(z, 1:q), tt)
  else warning("LongAR::error! invalid order p,q")
  
  outmodel
}

# a method for returning model's AIC (Akaike's Information Criterion) and BIC (Bayesian Information Criterion)
# with a changeable parameter
InfCrit = function(ml_model, type="AIC") {
  if (type=="AIC") {
    AIC(ml_model)
  } else if (type=="BIC") {
    BIC(ml_model)
  } else {
    warning("Invalid type (AIC or BIC)", call=F)
  }
}


#determine the model's AIC using an inbuilt function ar()

AICs <- ar(temp_test$Tres, order.max=kmax)$aic

# find minimal AIC for pmax <= p <= kmax
minAIC <- min(AICs[pmax:kmax])
for (k in pmax:kmax) {
  if (AICs[k] == minAIC) {
    korder <- k
    break
  }
}

k <- korder

models.arma <- list() 

suppressMessages(pkgTest("dynlm"))
models.arma[[paste(1,0,sep=",")]] <- LongAR(temp_test$Tres, k, 1, 0)
models.arma[[paste(0,1,sep=",")]] <- LongAR(temp_test$Tres, k, 0, 1)

for(p in 1:pmax) {  
  for(q in 1:qmax) {
    models.arma[[paste(p,q,sep=",")]] <- LongAR(temp_test$Tres, k, p, q)
  }
}

bic <- cbind(
  BIC = sapply(models.arma, function(x) InfCrit(x, type="BIC")),
  AIC = sapply(models.arma, function(x) InfCrit(x, type="AIC"))
)
bic <- bic[order(bic[,"BIC"]),]

head(bic, n=5)
```

Now we have 5 ARMA models with the lowest BIC with orders `"p,q"`.

```{r}
bestArma <- head(bic, n=5)
orders <- rownames(bestArma)
topModels <- list() # here I put the top models marked by their orders "p,q" as a key

for (j in 1:length(orders)) {
  key <- orders[[j]]
  topModels[[key]] <- models.arma[[key]]
}

( bestArma <- cbind(bestArma, topModels) ) #checking if the list contains models
#and then accessing them in the following way:
bestArma[[1,3]] #first in the list and the third column for the model itself
```

### 4.2. Adjusting the Regression Coefficients Using the Maximum Likelihood Estimate
 
The Maximum Likelihood Estimate (MLE) optimizes the, so called, likelihood-function with respect to the regression coefficients. One can find estimates using the Residual Square Sum (RSS). We will optimize the model parameters via an inbuilt function arima, specifying parameter `method = "ML"` and also using the former `dynlm`-type model and directly calculating the RSS.

```{r}
MaxLikelihoodOptim = function (tmpmodel, use_arima=TRUE) {
  params <- names(tmpmodel$coefficients)
  ( p <- sum(grepl("x",params)) )
  ( q <- sum(grepl("z",params)) )
  
  pars0 <- tmpmodel$coefficients
  
  #inbuilt function
  if (use_arima) {
    coefs <- arima(temp_test$Tres, order = c(p, 0, q), 
                   init=c(pars0[-1], pars0[1]), method = "ML", transform.pars = F)$coef
    
    # making the intercept coefficient first in the list
    intersect <- coefs[length(coefs)]
    coefs <- coefs[1:(length(coefs)-1)]
    coefs <- c(intersect, coefs)
    
    return(coefs)
  } else {
    #other version
    qmp <- max(0,q - p)
    x <- as.numeric(temp_test$Tres)
    ntest <- length(x)
    
    #residual square sum
    RSS = function(pars) {
      z <- rep(0,length(x) + qmp)
      for(i in (p + 1):ntest) {
        xz <- c(1, x[i:(i - p)], z[(i:(i - q)) + qmp])
        z[i + qmp] <- x[i] - c(append(head(pars, n = p + 1), 0, after = 1), 0, tail(pars, n = q)) %*% xz
      }
      z <- z[-(1:(p + qmp))]
      sum(z^2)
    }
    
    optim(pars0, RSS)$par
  }
}

#model comparison function
CompareModels = function(models) {
  orders <- rownames(models)
  
  output <- list()
  
  for (i in 1:length(orders)) {
    HannanRissanen_Result <- models[[i,3]]$coefficients
    MLE_using_ARIMA <- MaxLikelihoodOptim(models[[i,3]])
    MLE_not_ARIMA <- MaxLikelihoodOptim(models[[i,3]], use_arima=F)
    output[[ orders[[i]] ]] <- cbind(HannanRissanen_Result, MLE_using_ARIMA, MLE_not_ARIMA)
  }
  
  output
}

( comparison <- CompareModels(bestArma) )

#changing model coefficients
OptimizeModels = function (models, use_arima=F) {
  orders <- rownames(models)
  
  for (i in 1:length(orders)) {
    if (use_arima) {
      result_coeffs <- MaxLikelihoodOptim(models[[i,3]])
    } else {
      result_coeffs <- MaxLikelihoodOptim(models[[i,3]], use_arima=F)
    }
    
    models[[i,3]]$coefficients <- result_coeffs
    
  }
  models
}

newBestArma <- OptimizeModels(bestArma, use_arima=F)

# checking if the coefficients were changed
bestArma[[5,3]]$coefficients
newBestArma[[5,3]]$coefficients
```

### 4.3. Plotting the Resulting Models With their Residuals

```{r plot11csaa, include=T, fig.width=9, fig.height=6}
par(mfrow=c(2,2))

for (i in 1:5) {
  plot(temp_test$Tres, type="p", main=paste("ARMA(",orders[[i]],")"), ylab="T")
  lines(newBestArma[[i,3]]$fitted.values, col="blue")
  plot(newBestArma[[i,3]]$residuals, main=paste("ARMA(",orders[[i]],") residuals"), ylab="res")
}
```

### 4.4. Plotting the Resulting Models In the Original Time Series

```{r plot1212, fig.width=8, fig.height=4}
par(mfrow=c(1,1))
n <- length(temp_test$Tres)
for(i in 1:5) {
  o <- unlist(strsplit(orders[[i]],",")); p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]])
  m <- length(newBestArma[[i,3]]$fitted.values)
  plot(temp$T[1:m], type="p", main=paste("Systematic lm + ARMA(",orders[[i]],")"), xlab="day",ylab="T")
  
  syst_fit <- model.sCos$fitted.values[1:m]
  lines(syst_fit, col="red", lwd=2)
  lines(newBestArma[[i,3]]$fitted.values + syst_fit, col="blue", lwd=2)
  legend("topleft", legend=c("(1): Seasonal", paste("(2): (1) + ARMA(",orders[[i]],")")),
         col=c("red","blue"), lty=1,lwd=2 , cex=0.75)
}
```

The Hannan-Rissanen procedure with maximum orders: `kmax = 20`, `pmax = 10`, and `qmax = 10`, determined the optimal ARMA models to be: 

```{r}
row.names(newBestArma)
```
Combining the systematic components, i.e.: the seasonal and cyclical components, with the linear regression of a given ARMA model we have found 5 most accurate linear models for the test part of the original time series. <br/>
  We can write the resulting regression of an ARMA model: <br/>
  
  $\hat{x}_t = \hat{\varphi}_0 + \hat{\varphi}_1 x_{t - 1} + ...  + \hat{\varphi}_p x_{t - p} + \hat{\theta}_1 z_{t - 1} + ... + \hat{\theta}_{t - q} z_{t - q} + \hat{\varepsilon}_t$ <br/>
  
  which corresponds to:
  
  $\hat{x}_t = \hat{\varphi}_0 + \hat{\varphi}_1 x_{t - 1} + \hat{\varepsilon}_t$ <br/>
  
  $\hat{x}_t = -0.02785883 + 0.64562578 x_{t - 1} + \hat{\varepsilon}_t$, <br/> $\sigma_{\hat{\varepsilon}_t}^2 = 2.475145$ <br/>
  
  in case of the very first $ARMA(1,0)$ model, and:
  
  $\hat{x}_t = \hat{\varphi}_0 + \hat{\varphi}_1 x_{t - 1} + \hat{\theta}_1 z_{t - 1} + \hat{\theta}_2 z_{t - 2} + \hat{\varepsilon}_t$ <br/>
  
  $\hat{x}_t = -0.02631937 + 0.68694854 x_{t - 1}  -0.01196797 z_{t - 1} -0.08960355 z_{t - 2} + \hat{\varepsilon}_t + \hat{\varepsilon}_t$, <br/> $\sigma_{\hat{\varepsilon}_t}^2 = 2.444121$ <br/>
  
  for the second $ARMA(1,2)$.
  
  For demonstration we will combine the seasonal fit $\hat{x}_t^{S} = \hat{C}_0 + \hat{C}_1 \cos{(2 \pi t / 365)} + \hat{C}_2 \sin{(2 \pi t / 365)}$ with the ARMA fit $\hat{x}_t$ of the first model:
  

  $\hat{x}_t^{S} + \hat{x}_t = \hat{\psi}_0 + \hat{C}_1 \cos{(2 \pi t / 365)} + \hat{C}_2 \sin{(2 \pi t / 365)} + \hat{\varphi}_1 x_{t - 1} + \hat{\varepsilon}_t$,<br/> $\hat{\psi}_0 = \hat{C}_0 + \hat{\varphi}_0$
  
  $\hat{x}_t^{S} + \hat{x}_t = 9.290926 - 4.2990 \cos{(2 \pi t / 365)} - 3.5194 \sin{(2 \pi t / 365)} + 0.64562578 x_{t - 1} + \hat{\varepsilon}_t$,<br/> $\sigma_{\hat{\varepsilon}_t}^2 = 2.475145$  <br/>

The evaluation part will be used in the following section where we will test the given models and carry out predictions.
 
## 5. ARMA Model Diagnostics and Predictions
 
Prior to carrying out (single-step and multiple step) predictions, we need to test the accuracy of the given models. For that we use the following series of diagnostic tests:
 - Zero Autocorrelation
 - Normality of Residues
 - Conditional Heteroskedasticity
 
### 5.1. Zero Autocorrelation
 
The Autocorrelation function (ACF) shows a reasonably clear image of the dependency of individual random variables with respect to time lag. This means that if the ACF has non-zero values for some lags greater than zero, then it is likely that the residues after removing the ARMA regression's fitted values still contain some systematic or ARMA component. To find out we test for a null hypothesis: $ACF(k) = 0$ for all $k > 0$, against an alternative $ACF(k) \neq 0$ for some $k > 0$.

```{r plot1zaa, fig.width=9, fig.height=4}
ZeroACF = function (x, zeroValue) {
  x = ifelse (abs(x) < zeroValue, 0, x)
}

NonZeroValues = function (series, zero) {
  indices <- which(series != 0)
  nonzeros <- list()
  
  nonzeros[["zero val"]] <- zero
  
  for (i in 2:length(indices)) {
    nonzeros[[paste("k",indices[i], sep = '_')]] <- series[indices[i]]
  }
  
  data.frame(head(nonzeros))
}

ACFs <- list()
Nonzeros <- list()

par(mfrow=c(1,2))
for (i in 1:5) {
  acf(newBestArma[[i,3]]$residuals, ylab="ACF", lag.max=nt, main=paste("ARMA(",orders[[i]],") res"))
  
  ACFs[[i]] <- acf(newBestArma[[i,3]]$residuals, ylab="ACF", lag.max=nt, plot=F)$acf
  zero <- 2 / sqrt(nt)
  ACFs[[i]] <- sapply(ACFs[[i]], function(x) ZeroACF(x, zero))
  lines(ACFs[[i]], col="red", lwd=2)
  
  Nonzeros[[i]] <- NonZeroValues(ACFs[[i]], zero)
}
```

Using the zero-value approximation: $2/\sqrt{n_t}$, where $n_t$ (`nt`) is the length of the residual time series, we notice that there seems to be a resilient 83-day lag which was not removed (even after including the significant residual lags such as 45, and 83 in the `model.sCos` in section 3). The significance of the 83-day lag, however, is not nearly as high as it would have otherwise been (with additional significant lags) if additional lags had not been introduced prior to carrying out the Hannan-Rissanen procedure. Thus we can assume that all models, having no other significant lags than the 83-day lag, describe the behavior of the test part of the residual time series reasonably well. In the following list:

```{r}
Nonzeros
```

we see the exact lags for which the ACF exceeds the zero-value.
 
### 5.2. Normality of Residues
 
If a given ARMA model describes the time series well enough, the residues should behave as if they were generated by a stochastic process with values at individual times $t$ being the realizations of independent identically distributed random variables. For sufficiently large number of time steps, the residues are assumed to be the results of a normally distributed random variable. 
 To test that, we use the, so called, Jarque-Bera test:

```{r JBTest}
JBToutput <- list()

for (i in 1:5) {
  JBToutput[[i]] <- tseries::jarque.bera.test(newBestArma[[i,3]]$residuals)
  
  pValue <- as.numeric(JBToutput[[i]]$p.value)
  
  JBToutput1 <- c("1,0",JBToutput[[i]]$statistic, JBToutput[[i]]$parameter, pValue)
  names(JBToutput1) <- c("p,q","statistic", "DOF", "p.value")
  data.frame(head(JBToutput1))
  if (pValue > 0.05) {
    message(paste("ARMA(",orders[[i]],") ::: p-value = ", round(pValue, 4),", Normality hypothesis rejected"))
  } else { message(paste("ARMA(",orders[[i]],") ::: p-value = ", round(pValue, 4),", Normality hypothesis accepted")) }
}
```

As it appears, the normality of the residues of all top 5 ARMA models is accepted. 

### 5.3. Conditional Heteroskedasticity
 
The term "heteroskedasticity" of a time series describes the tendency of its variance to change with time. A time series whose variance (up to a given time) remains the same are called "homoskedastic". The following Breusch-Pagan test assesses the homoskedasticity of a given model:

```{r Skedast}
alpha = 0.05
SkedtestResults <- list()
suppressMessages(pkgTest("lmtest"))
for (i in 1:5) {
  (SkedtestResults[[i]] <- lmtest::bptest(newBestArma[[i,3]]) )
  
  pValue <- as.numeric(SkedtestResults[[i]]$p.value)
  
  if (pValue > alpha) {
    message(paste("ARMA(",orders[[i]],") ::: p-value = ", pValue,", Null hypothesis rejected: model is heteroskedastic"))
  } else {
    message(paste("ARMA(",orders[[i]],") ::: p-value = ", pValue,", Null hypothesis accepted: model is homoskedastic"))
  }
} 
```

Thus on the 5% significance level we accept the null hypothesis for all models, i.e.: all models are heteroskedastic.

 -----------------------------------------------------------------------------------------------------------------

### 5.4. Forecasting and Single-Step Predictions
 
Now we make use of the evaluation part of the time series which we have separated from the original series earlier in section 4. Since the `dynlm` model we used earlier does not support `predict()` function from the `forecast` library we use equivalent $ARIMA(p,0,q)$ models instead of $ARMA(p,q)$, which are returned by an inbuilt function `arima()`, the result of which is an object that can be processed by the `predict()` call.

```{r predictplot1, fig.width=9, fig.height=4}
( neval <- length(temp_eval$Tres) )

par(mfrow=c(1,2))
suppressMessages(pkgTest("forecast"))
eval.predictions <- list()
merrors <- list()

for (i in 1:5) {
  # extract model orders from the top models from Hannan-Rissanen:
  o <- unlist(strsplit(orders[[i]],",")); p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]])
  # estimate a suitable model on the test part of the time series
  model_estim <- arima(temp_test$Tres, order=c(p,0,q))
  # use the estimated model as an argument, fitting the same model to the whole time series without re-estimating any parameters
  # taking the fitted seasonal compnent into account
  eval.predictions[[i]] <- Arima(model.sCos$residuals, order=c(p,0,q), model=model_estim, xreg=model.sCos)
  # and take the evaluation part for examination
  eval.predictions[[i]] <- tail(fitted(eval.predictions[[i]]), neval)
  merrors[[i]] <- mean((temp_eval$Tres - eval.predictions[[i]])^2)

  plot(x=temp_eval$time, y=temp_eval$T, 
       main=paste("Predictions ARMA(",p,",",q,"), RMSE = ", round(sqrt(merrors[[i]]), 4)),
       xlab="day", ylab="[°C]", type="l", lwd=1.5)
  lines(x=temp_eval$time, y=(eval.predictions[[i]] + temp_eval$sCos), col="blue", lwd=2)
  legend("bottomleft", legend=c("prediction", "actual values"),
         col=c("blue","black"), lty=1,lwd=2 , cex=0.75)
}
```

We have also computed Root Mean Square Errors for each model.

```{r}
minE <- which.min(merrors)
```

### 5.5. Comparison of Predictive Abilities Using the Diebold-Mariano Test
 
Taking two different ARMA models, the null hypothesis of the test states that both models have equal predictive abilities. We will be comparing 5 different models against each other, which will essentially amount to testing the predictive abilities of 10 distinct pairs of models. Practically, the test results can be expressed in a $5 \times 5$ "DieboldMarianoMatrix" with values $0$ (if model $i$ has the same predictive abilities as model $j$), $1$ (if model `i` has better predictive abilities than model $j$), and $-1$ (the other way around):

```{r}
DieboldMarianoMatrix <- matrix(0, ncol=5, nrow=5)
pvalueMatrix <- matrix(1, ncol=5, nrow=5)
for(i in 1:length(eval.predictions)) {
  for(j in 1:length(eval.predictions)) {
    if(i==j) next
    test <- forecast::dm.test(temp_eval$Tres - eval.predictions[[i]], temp_eval$Tres - eval.predictions[[j]])
    DieboldMarianoMatrix[i,j] <- (test$p.value < alpha) * sign(test$statistic) 
    pvalueMatrix[i,j] <- round(test$p.value, 3)
  }
}
DieboldMarianoMatrix

# pvalueMatrix
```

As the results of the Diebold-Mariano test suggest, no model has significantly better predictive abilities on a 5% significance level.
A quantitative characteristic of predictive ability is the Root Mean Square Error (RMSE). Here we can see how the predictions match the observed values from the evaluation part of the time series:

```{r predictPlot2, fig.width=9, fig.height=6}
par(mfrow=c(1,1));
o <- unlist(strsplit(orders[[minE]],",")); p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]])
n <- length(model.sCos$fitted.values)
fullpredict <- as.numeric(eval.predictions[[minE]] + temp_eval$sCos)
plot(x=temp$time, y=temp$T, main=paste("Prediction + Systematic , ARMA(", orders[[minE]],"):"), ylab="T",xlab="day")
lines(x=temp_eval$time, y=fullpredict, col="blue", lwd=2)

k = max(p, q)
syst_fit <- model.sCos$fitted.values[(k + 1):nt]
test_model <- newBestArma[[minE,3]]$fitted.values + syst_fit

lines(x=temp_test$time[(k + 1):nt], y=test_model, col="red",lwd=2)
lines(x=temp$time[nt:(nt + 1)], y=c(test_model[nt - 1], fullpredict[1]), col="red", lwd=2) # line connecting the test and eval datasets
legend("topleft", legend=c(paste("Model ARMA(",orders[[minE]],")"), paste("Model ARMA(",orders[[minE]],") prediction")),
       col=c("red","blue"), lty=1,lwd=2 , cex=0.75)
```

The model with the smallest $RMSE$ is $ARMA(1,0)$. Except for the outlying values, the predictive capabilities of the model can also be verified visually from the plot above. 
 
### 5.6 Multiple-Step Predictions

Predicting one step ahead of data can be done naiively by fitting and ARMA model on the entire time series (both test and evaluation segments). Once we reach the end of the dataset, however, we must predict with an increasing amount of uncertainty. This can be done by recursive accumulation of the past values (based on AR an MA parameters). A `predict()` method for an `arima` (or `Arima`) fit can perform this task for an arbitrary forecast horizon. The `forecast` package contains a method (with the same name) which also includes confidence levels (80% and 95% as default) enclosing the predicted values which tend towards a stable mean. The confidence intervals can grow with increasing steps, especially for models with higher MA degrees.

In this section we compare the multiple-step predictions (with the length of the evaluation part) for the top 5 ARMA models:

```{r multipredictPlot, fig.width=12, fig.height=4}
require(forecast)
par(mfrow=c(1,1))
ne <- length(temp_eval$time)
x <- as.numeric(temp_test$Tres)
X <- as.numeric(model.sCos$residuals)

for (i in 1:5) {
  o <- unlist(strsplit(orders[[i]],","));
  p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]]);
  key <- paste("(",p,",",q,")", sep="")
  
  model_estim <- arima(x, order=c(p,0,q))
  model <- Arima(X, order=c(p,0,q), model=model_estim, xreg=model.sCos) # use the seasonal regression for xreg
  
  xp <- tail(fitted(model), ne)
  fc <- forecast(model_estim, h=ne)
  
  xp_res <- temp_eval$Tres - xp
  rmse <- sqrt(mean(xp_res^2))
  rmse_h <- sqrt(mean((fc$mean - xp)^2))
  
  fc$mean <- fc$mean + temp_eval$sCos
  fc$lower <- fc$lower + temp_eval$sCos
  fc$upper <- fc$upper + temp_eval$sCos
  fc$x <- fc$x + temp_test$sCos
  xp <- xp + temp_eval$sCos
  plot(fc, lwd=1, xlab="day", ylab="[°C]", main=paste("ARMA", key, sep=""), xlim=c(floor(0.6*length(x)), length(X)))
  mtext(paste(" RMSE(1) = ", round(rmse, digits=3), " ,  RMSE(", ne, ") = ", round(rmse_h, digits=3), sep=""))
  lines(x=(nt + 1):(nt + ne), y=xp, lwd=2, col="green4", type="l")
  lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="red", type="p", pch=20)
  legend("bottomleft", legend=c(paste("prediction (", ne,"-step)", sep=""), "1-step prediction","data"),
         col=c("blue", "green4","red"), lty=c(1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, 20))
}
```

Now even though we have no validation for our models for $t > n$ where $n$ is the length of the time series, we can use our models to make a forecast for another year (more precisely, year 2016):

```{r futurePredictPlot, fig.width=12, fig.height=4}
  i <- 1
  o <- unlist(strsplit(orders[[i]],","));
  p <- as.numeric(o[[1]]); q <- as.numeric(o[[2]]);
  key <- paste("(",p,",",q,")", sep="")

  # forecast for another year (2016)
  n_future = 365
  model_future <- arima(X, order=c(p,0,q))
  fc_future <- forecast(model_future, h=n_future)
  future_sCos <- sapply((nt + ne):(nt + ne + n_future - 1), 
                        function(t) c(1, cos(2*pi*t/seasons[1]), sin(2*pi*t/seasons[1])) %*% model.sCos$coefficients)
  
  fc_future$mean <- fc_future$mean + future_sCos
  fc_future$lower <- fc_future$lower + future_sCos
  fc_future$upper <- fc_future$upper + future_sCos
  fc_future$x <- fc_future$x + model.sCos$fitted.values
  
  plot(fc_future, lwd=1, xlab="day", ylab="[°C]", main=paste("ARMA", key, sep=""))
  legend("topleft", legend=c("prediction","data"), col=c("blue", "black"), lty=c(1, 1),lwd=2 , cex=0.8)
```

And unless we take global warming (a possible trend), for example, into account we can assume this prediction to be valid. Since our dataset is only a year long we are unable to make any assumptions about possible long-term changes in temperature.

 --------------------------------------------------------------------------------------------------------------
 
## 6. Non-Stationary Models
 
ARMA type models can be used when we safely assume stationarity of the time series. Namely, when the series has a constant variance its mean values and covariances only depend on the lag, not on time. Although ARMA models seem to sufficiently describe the given time series, we may want to try out the so called Integrated processes.
 
### 6.1. Diagnostic Tests
 
First we will test whether the original time series after extracting the seasonal component has the properties of a non-stationary process. The most common approach involves testing for unit-roots of the process' characteristic polynomial. We begin by testing the residuals after filtering seasonal components via the Dickey-Fuller (DF) test.
 
#### 6.1.1 Dickey-Fuller Test (DF)
 
The null hypothesis of the DF test states: $x_t = x_{t-1} + e_t$, that is, the time series is a realization of a random walk process. It is carried out by setting $\Delta x_t = x_t - x_{t-1} = \rho \ x_{t-1} + e_t$ where $e_t$ is an i.i.d process. It suffices to show that the $\rho$ parameter of such regression equals zero. In general, the regression may assume $\Delta x_t = \mu + \delta \ t + \rho \ x_{t-1} + e_t$ with additional parameters $\mu$ (mean value) and $\delta$ (deterministic drift).

```{r}
x <- temp_test$Tres
x <- as.numeric(x)
dx <- diff(x) # first differences
m <- length(dx)

xt1 <- x[1:m]
tt <- 1:m
reg <- lm(dx ~ xt1 + 1 + tt)
reg.sum <- summary(reg)
reg.sum$coefficients
```

And after the regression of the difference time series we proceed to compute the $DF$-statistic from the `rho` parameter:

```{r}
rho <- reg.sum$coefficients[2,1]
sigma_rho <- reg.sum$coefficients[2,2]
( DFstat <- rho / sigma_rho )
```

The value has to be compared with the critical values for the `DF`-statistic, whose distribution is estimated via simulations
 for particular sample sizes:

```{r, echo=T}
critVals <- -c(3.60, 3.50, 3.45, 3.43, 3.42, 3.41) # for alpha = 0.05
sampleSizes <- c(25, 50, 100, 250, 500, 100000) # sample size
```

```{r}

( critVal <- approx(sampleSizes, critVals, m, rule=2)$y )

if (DFstat >= critVal) {
  message(paste(" DF = ", round(DFstat, digits=4), " > ", critVal ,", rho = ", round(rho, digits=4),
                " ::: Null Hypothesis accepted. Possible stochastic trend."))
} else {
  message(paste(" DF = ", round(DFstat, digits=4), " < ", critVal, ", rho = ", round(rho, digits=4),
                " ::: Null Hypothesis rejected. Stochastic trend insignificant."))
}
```

The presence of a stochastic trend for process $\Delta x_t = \mu + \delta \ t + \rho \ x_{t-1} + e_t$ is rejected on a 5% significance level. This means that the residual time series after extracting the seasonal deterministic should not be modelled as an integrated process. We can compare the results with the inbuilt `adf.test` function from `tseries` package:

```{r}
require(tseries)
adf.test(x, k = 0) # k = 0 corresponds to lag = 1
```


#### 6.1.2 Augmented Dickey-Fuller Test (ADF)
 
The same procedure can be generalized for lags $k > 0$ as well. For that we can just use the method from `tseries` package:

```{r}
DFi <- list()
for (i in 1:5) DFi[[i]] <- suppressWarnings(adf.test(temp_test$Tres, k = i)$statistic)
DFi <- data.frame(matrix(unlist(DFi), nrow=length(DFi), byrow=T))
names(DFi) <- c("DF")
DFi
```

```{r, eval=F}
# The ADF method sets a default lag value to 
( k <- trunc((length(x)-1)^(1/3))  )

adf.test(x)
```

#### 6.1.3 Phillips-Perron Test
 
Similarily to the DF test, the PP-test is, again, a unit-root test which relies on regressing $\Delta X \sim \mu + \delta \ t + \rho X_{t - 1}$. While the ADF test adresses the issue of the time series being potentially generated by a process of higher orders by using lags $k = 1, 2, ...$ and expanding the regression equation, PP-test makes a non-parametric correction to the t-statistic.
 
```{r}
pp.test(x)
```

This makes the PP-test robust with respect to unspecified autocorrelation and heteroskedasticity.
 
#### 6.1.4 KPSS Test
 
Published by Kwiatkovski, Philips, Schmidt, and Shin (1992), another test approaches the stationarity-vs-non-stationarity issue from an opposite point of view. Using KPSS we can test for stationarity hypothesis (about a deterministic trend). Contrary to the previously used tests, the presence of a unit root is the alternative. It can be used in cases when unit roots hypothesis cannot be rejected and the time series coult be non-stationary (or we have a sample of insufficient length). The test relies on testing for zero variance of an i.i.d process present in the stochastic trend part in $x_t \sim ST_t + \epsilon_t$, or alternatively with deterministic trend: $x_t \sim ST_t + \delta \ t + \epsilon_t$. 

```{r}
x <- as.vector(x, mode="double")
n <- length(x)
ntrend.reg <- lm(x ~ 1)
summary(ntrend.reg)
e <- residuals(ntrend.reg)
table <- c(0.739, 0.574, 0.463, 0.347) # KPSS table for regression without drift

tablep <- c(0.01, 0.025, 0.05, 0.10)
s <- cumsum(e)
eta <- sum(s^2)/(n^2)
s2 <- sum(e^2)/n

l <- 3 # truncation lag parameter
s2 <- .C("tseries_pp_sum", as.vector(e, mode="double"), as.integer(n), as.integer(l), s2=as.double(s2))$s2
kpssStat <- eta/s2
pval <- approx(table, tablep, kpssStat, rule=2)$y
c(KPSS=kpssStat, pval=pval)
```

As it appears, the stationarity null hypothesis is accepted. With `pvalue > 0.1`. The same result can be obtained using the `kpss.test` function from the `tseries` package:

```{r}
( testNDrift <- kpss.test(x, null="Level") )
( testDrift <- kpss.test(x, null="Trend") )
```

where the second result corresponds to testing the second type of regression $x \sim \delta \ t$ with trend, which shows that we can accept the stationarity hypothesis with p-value 

```{r}
testDrift$p.value
```

By regressing the given trend

```{r}
t <- 1:n
trend.reg <- lm(x ~ t)
summary(trend.reg)
e <- residuals(trend.reg)
table2 <- c(0.216, 0.176, 0.146, 0.119) # KPSS statistic table for regression with drift
s <- cumsum(e)
eta <- sum(s^2)/(n^2)
s2 <- sum(e^2)/n
s2 <- .C("tseries_pp_sum", as.vector(e, mode="double"), as.integer(n), as.integer(l), s2=as.double(s2))$s2
kpssStat <- eta/s2
pval <- approx(table2, tablep, kpssStat, rule=2)$y
c(KPSS=kpssStat, pval=pval)
```

we reject the stationarity about a trend null hypothesis. The test can also be carried out for larger values of truncation lag parameter:

```{r}
kpss.test(x, null="Level", lshort = F)
kpss.test(x, null="Trend", lshort = F)
```

### 6.2 Identifying Models of Integrated Processes

The diagnostic tests carried out in the previous section suggest that the residuals `temp_test$Tres` are generated by a stationary stochastic process. 
  The `auto.arima` method from the `forecast` package searches through a multitude of combinations of parameters, just like we did in section 4 when searching for the best ARMA model:
  
```{r, echo=T}
auto.arima(x, ic = "bic")
```

The result corresponds to the best ARMA model, and thus confirms the validity of our approach in section 4.

The original time series without the deterministic annual wave caused by the Earth's tilt during orbit around the Sun could still be modeled as a non-stationary process (about some deterministic wave).
  If we apply the same `auto.arima` method on the original time series without deterministic seasonal component:

```{r}
x <- as.numeric(temp_test$T)
auto.arima(x, ic = "bic")
```

This means that the original temperature data can be modeled as an ARIMA process with $d = 1$.
 
#### 6.2.1 ARIMA
 
Unlike the method from the `forecast` package, we will not consider seasonality in our data, particularly due to the fact that it does not even contain the whole annual set of observation. Modeling seasonality through a SARIMA model would require at least two periods in the test part of the time series alone.
    For the purpose of this presentation we search through a $5 \times 2 \times 5$ grid of $(p, d, q)$ triples:

```{r}
pmax = 5; qmax = 5; dmax = 2;
models.arima <- list();
for (p in 0:pmax) {
  for (q in 0:qmax) {
    for (d in 0:dmax)  {
      tmp <- tryCatch(arima(x, order=c(p,d,q)), error=function(e) e, warning=function(w) w)
      if (length(tmp$message) == 0) {
        key <- paste(p,",",d,",",q)
        models.arima[[key]] <- c(p=p, d=d, q=q, rss=tmp$sigma2, AIC=tmp$aic, BIC=InfCrit(tmp, type="BIC"))
      }
    }
  }
}

models.arima <- data.frame(matrix(unlist(models.arima), nrow=length(models.arima), byrow=T))
names(models.arima) <- c("p", "d", "q", "RSS", "AIC", "BIC")
head(models.arima)
```

And like we did in section 4, after getting a large set of models of varying quality, we sort the resulting list according to their $BIC$'s:

```{r}
bic <- models.arima[6]
bestArima <- models.arima[order(bic),]
head(bestArima)
```
 
#### 6.2.2 ARFIMA
 
So far we have restricted ourselves to searching through parameter spaces of smaller lag order values (for the sake of reducing computation time). There is however, an alternative for modelling ARIMA-type processes with long memory. This may assess the fact that the nonzero values of the data's ACF still appear after 30 steps.

```{r}
suppressMessages(pkgTest("fracdiff"))
pmax = 5; qmax = 5; 
models.arfima <- list()
for (p in 0:pmax) {
  for (q in 0:qmax) {
    tmp <- tryCatch(fracdiff(x, nar=p, nma=q), error=function(e) e, warning=function(w) w)
    if (length(tmp$message) == 0) {
      d <- tmp$d
      key <- paste(p,",",d,",",q)
      models.arfima[[key]] <- c(p=p, d=d, q=q, rss=tmp$sigma, BIC=(log(n) * (p + q + 1) - 2 * tmp$log.likelihood))
    }
  }
}

models.arfima <- data.frame(matrix(unlist(models.arfima), nrow=length(models.arfima), byrow=T))
names(models.arfima) <- c("p", "d", "q", "RSS", "BIC")
head(models.arfima)

bic <- models.arfima[5]
bestArfima <- models.arfima[order(bic),]
head(bestArfima)
```
 
### 6.3 Predictive Properties of the Chosen Models
 
```{r predictComparison, fig.width=10, fig.height=4.5}
X <- as.numeric(temp$T)

par(mfrow=c(1,1))
plot_models <- vector()

model1_id <- 1;
p = bestArima[model1_id, 1]; d = bestArima[model1_id, 2]; q = bestArima[model1_id, 3];
key <- paste("(",p,",",d,",",q,")", sep="")
name <- paste("ARIMA", key, sep="")
plot_models[1] <- name

ARIMA_predict <- tail(fitted(Arima(X, order=c(p,d,q), model=arima(x, order=c(p,d,q)))), ne)

model2_id <- 2;
p = bestArima[model2_id, 1]; d = bestArima[model2_id, 2]; q = bestArima[model2_id, 3];
key <- paste("(",p,",",d,",",q,")", sep="")
name <- paste("ARIMA", key, sep="")
plot_models[2] <- name

ARIMA_predict1 <- tail(fitted(Arima(X, order=c(p,d,q), model=arima(x, order=c(p,d,q)))), ne)


model4_id <- 1;
p = bestArfima[model4_id, 1]; d = bestArfima[model4_id, 2]; q = bestArfima[model4_id, 3];
key <- paste("(",p,",",round(d,2),",",q,")", sep="")
name <- paste("ARFIMA", key, sep="")
plot_models[3] <- name

ARFIMA_predict <- tail(fitted(arfima(X, drange=c(0, d), model=fracdiff(x, nar=p, nma=q))), ne)

x_test <- x

colors <- c(hsv((9.5 - 2) / 8), hsv((9.5 - 3) / 8), hsv((9.5 - 5) / 8))

plot(x_test, type="l", 
     xlim=c(floor(0.9*length(x)), length(X)), ylim=c(3.5, 17),
     main="1-step predictions of chosen models", xlab="day", ylab="[°C]")
lines(x=nt:(nt + 1), y=temp$T[nt:(nt + 1)])
lines(x=(nt + 1):(nt + ne), y=ARIMA_predict, lwd=2, col=colors[1], type="l")
lines(x=(nt + 1):(nt + ne), y=ARIMA_predict1, lwd=2, col=colors[2], type="l")
lines(x=(nt + 1):(nt + ne), y=ARFIMA_predict, lwd=2, col=colors[3], type="l")

lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="black", type="p", pch=1)
legend("bottomleft", legend=c(plot_models,"data"),
       col=c(colors, "black"), lty=c(1, 1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, NA, 1))
```

We also consider the predictive properties of the chosen set of models. We will be comparing the top 6 models from each type.
 
#### 6.3.1 ARIMA

```{r arimaPreds, fig.width=12, fig.height=5}
errors <- list()
xp_resids <- list()

library(forecast)
par(mfrow=c(1,2))
for (i in 1:6) {
  p = bestArima[i,1]; d = bestArima[i,2]; q = bestArima[i,3];
  key <- paste("(",p,",",d,",",q,")", sep="")
  name <- paste("ARIMA", key, sep="")

  model_estim <- arima(x, order=c(p,d,q))
  model <- Arima(X, order=c(p,d,q), model=model_estim)

  xp <- tail(fitted(model), ne)
  fc <- forecast(model_estim, h=ne)
  
  xp_res <- temp_eval$T - xp
  rmse <- sqrt(mean(xp_res^2))
  rmse_h <- sqrt(mean((fc$mean - xp)^2))
  errors[[name]] <- c(model=name, RMSE=round(rmse, 5), RMSE_ne=rmse_h, BIC=round(bestArima[i, 6], 3))
  xp_resids[[name]] <- xp_res

  plot(fc, lwd=1, xlab="day", ylab="[°C]", main=name, xlim=c(floor(0.6*length(x)), length(X)))
  mtext(paste(" RMSE(1) = ", round(rmse, digits=3), " ,  RMSE(", ne, ") = ", round(rmse_h, digits=3), sep=""))
  lines(x=(nt + 1):(nt + ne), y=xp, lwd=2, col="green4", type="l")
  lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="red", type="p", pch=20)
  legend("bottomleft", legend=c(paste("prediction (", ne,"-step)", sep=""), "1-step prediction","data"),
         col=c("blue", "green4","red"), lty=c(1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, 20))
}
```

#### 6.3.2 ARFIMA

```{r arfimaPreds, fig.width=12, fig.height=5}
library(fracdiff)

par(mfrow=c(1,2))
for (i in 1:6) {
  p = bestArfima[i, 1]; d = bestArfima[i, 2]; q = bestArfima[i, 3];
  key <- paste("(",p,",",round(d,2),",",q,")", sep="")
  name <- paste("ARFIMA", key, sep="")

  model_estim <- fracdiff(x, nar=p, nma=q)
  model <- arfima(X, drange=c(0, d), model=model_estim)
  
  xp <- tail(fitted(model), ne)
  fc <- forecast(model_estim, h=ne)
  
  xp_res <- temp_eval$T - xp
  rmse <- sqrt(mean(xp_res^2))
  rmse_h <- sqrt(mean((fc$mean - xp)^2))
  errors[[name]] <- c(model=name, RMSE=round(rmse, 5), RMSE_ne=round(rmse_h, 5), BIC=round(bestArfima[i, 5], 3))
  xp_resids[[name]] <- xp_res
  
  plot(fc, lwd=1, xlab="day", ylab="[°C]", main=name, xlim=c(floor(0.6*length(x)), length(X)))
  mtext(paste(" RMSE(1) = ", round(rmse, digits=3), " ,  RMSE(", ne, ") = ", round(rmse_h, digits=3), sep=""))
  lines(x=(nt + 1):(nt + ne), y=xp, lwd=2, col="green4", type="l")
  lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="red", type="p", pch=20)
  legend("bottomleft", legend=c(paste("prediction (", ne,"-step)", sep=""), "1-step prediction","data"),
         col=c("blue", "green4","red"), lty=c(1, 1, NA),lwd=2 , cex=0.8, pch=c(NA, NA, 20))
}
```


#### 6.3.4 Comparison of Predictive Properties

So far, we have chosen 6 best models from categories ARIMA and ARFIMA of integrated processes. These 12 possibilities (17 if we include ARMA models from section 4) have to be sorted according to their predictive properties, namely errors and DM test. Since an $12 \times 12$ (or $17 \times 17$ if we also include ARMA models) Diebold-Mariano matrix seems too large for practical purposes, we will choose 5 models from all  categories with the lowest `RMSE` of single-step predictions.

```{r}
# we can also include errors from top 5 ARMA list

for (i in 1:5) {
  name <- paste("ARMA(", orders[[i]],")", sep="")
  errors[[name]] <- c(model=name, RMSE=round(sqrt(merrors[[i]]), 5), RMSE_ne="-", BIC=round(as.numeric(bestArma[i, 1]), 3))
  xp_resids[[name]] <- temp_eval$Tres - eval.predictions[[i]]
}

xp_resids <- matrix(unlist(xp_resids), nrow=length(xp_resids), byrow=T)
errors <- data.frame(matrix(unlist(errors), nrow=length(errors), byrow=T))
names(errors) <- c("model", "RMSE(1)", paste("RMSE(", ne, ")", sep=""), "BIC")

# sort by RMSE(1)
rmse1 <- errors[2]
lowestRMSE1_resids <- xp_resids[order(rmse1),]
( lowestRMSE1 <- errors[order(rmse1),] )
```

And we can apply the DM test on first 6 models:

```{r}
npred_models <- 6
DieboldMarianoMatrix <- matrix(0, ncol=npred_models, nrow=npred_models)
pvalueMatrix <- matrix(1, ncol=npred_models, nrow=npred_models)
for(i in 1:npred_models) {
  for(j in 1:npred_models) {
    if(i==j) next
    res_i <- lowestRMSE1_resids[i,]; res_j <- lowestRMSE1_resids[j,];
    test <- forecast::dm.test(res_i, res_j)
    DieboldMarianoMatrix[i,j] <- (test$p.value < alpha) * sign(test$statistic) 
    pvalueMatrix[i,j] <- round(test$p.value, 3)
  }
}
DieboldMarianoMatrix

pvalueMatrix
```

We observe that there are no significant differences between models. Perhaps the 6th model seems to have the most significantly different predictive abilities from 3rd and 5th model. Significant differences appear only as we increase the dimension of the `DieboldMarianoMatrix` to 17 (all models). With ARIMA and some ARFIMA models having significantly better predictive abilities:

```{r}
better_than <- list()
npred_models <- 17
#DieboldMarianoMatrix <- matrix(0, ncol=npred_models, nrow=npred_models)
#pvalueMatrix <- matrix(1, ncol=npred_models, nrow=npred_models)
for(i in 1:npred_models) {
  for(j in 1:npred_models) {
    if(i==j) next
    res_i <- lowestRMSE1_resids[i,]; res_j <- lowestRMSE1_resids[j,];
    test <- forecast::dm.test(res_i, res_j)
    #DieboldMarianoMatrix[i,j] <- (test$p.value < alpha) * sign(test$statistic) 
    if (test$p.value < alpha && test$statistic > 0) {
      better_than[[as.character(i)]] <- c(model=as.character(lowestRMSE1[i, 1]), betterThan=as.character(lowestRMSE1[j, 1]))
    }
    #pvalueMatrix[i,j] <- round(test$p.value, 3)
  }
}
better_than <- data.frame(matrix(unlist(better_than), nrow=length(better_than), ncol=2, byrow=T))
names(better_than) <- c("model", "better than")
better_than
```

Thus according to Diebold-Mariano test, only $ARIMA(2, 1, 1)$ (which is second in BIC in our grid search and first in `auto.arima`) is better than $ARIMA(1,1,2)$ (third in grid search).
 
### 6.4 Conclusions
  
As the matter of fact, different types of models of integrated processes capture different properties of the time series. 
We decided not to use SARIMA models since the data does not contain at least two seasons, and also we refrained from using (G)ARCH type models since the residuals of `model.sCos` are conditionally homoskedastic and there is no regression to BP-test against in the case of the original time series temperature data.

  While ARIMA and ARFIMA models seem to account for much of the non-stationarity in the original time series, they have larger $BIC$ than their ARMA counterparts on the systematic residuals:
  
```{r}
bic <- as.numeric(paste(lowestRMSE1[,4]))

lowestBIC <- cbind(lowestRMSE1[order(bic), 1:2], lowestRMSE1[order(bic),4])
colnames(lowestBIC) <- c("model", "RMSE(1)", "BIC")
lowestBIC
```

However, it should be noted that the value of $BIC$ in the context of ARMA type models was evaluated on the residuals of a seasonal systematic fit without taking the systematic coefficients into account as parameters, whereas $BIC$'s of ARIMA and ARFIMA were fitted on the original data. Nevertheless the validity of the integrated models remains rather dubious, since we know that average daily temperature will change in more-or-less identical annual cycles, and these models were only used on original data as an exercise.

  The Diebold-Mariano test showed no significant difference in 1-step prediction property except for model $ARIMA(2, 1, 1)$, recognized as the best fit by `auto.arima` from `forecast` package.
  
  Thus quantitatively, ARMA models on residuals seem to outcompete the remaining categories of integrated processes on original data.

## 7. Exponential Smoothing Methods

Exponential smoothing techniques were among the first to appear (in 1950's), and were, thanks to being relatively fast, used in various predictions and smoothings of time series. Whereas in MA-type models the finite moving averages contribute to each point using regression extimation of their coefficients, the ES technique uses an exponential window, i.e.: a weighed moving average of all values which decreases exponentially into the past.

### 7.1 Choice of a Smoothing Technique for our Data

Each time series could potentially have three properties which can be smoothed, namely: <br/>

  1.) level (mean value) <br/>
  2.) trend (slope) <br/>
  3.) seasonality (containing weighed averages of each $L$ steps into the past) <br/>

The approach to each property can either be additive ($A$) or multiplicative ($M$). 
We will work with the residuals `temp_test$Tres` with extracted annual period, hence we will not model any seasonality, or trend. We are left with additive and multiplicative models of the exponential smoothing of mean values. The additive smoothing models are governed by the following set of recurrence equations:

$x_t = a_t + \epsilon_t$ (observation eqn') <br/>

$a_t = a_{t-1} + \alpha \epsilon_t$ (state space eqn'),  $\alpha \in [0, 1]$ <br/>

for an additive error, and  <br/>

$x_t = a_{t-1} (1 + \epsilon_t)$ (observation) <br/>

$a_t = a_{t-1} (1 + \alpha \epsilon_t)$ (state),  $\alpha \in [0, 1]$ <br/>

for a multiplicative error. In both cases the error can be expressed as $\epsilon_t = x_t - a_{t - 1} = x_t - \hat{x}_{t|t-1}$. Where 
$\hat{x}_{t|t-1}$ are prediction estimates from the previous step.
 The core idea behind simple exponential smoothing with additive error smoothing can be described using the following algorithm:

```{r}
simpleExpSmooth <- function(x, alpha) {
    n <- length(x)
    x.hat <- c(x[1], numeric(n - 1))
    for (i in 2:n) {  
      x.hat[i] <- alpha * x[i - 1] + (1 - alpha) * x.hat[i - 1]
    }
    
    res <- (x - x.hat)
    k <- 1 # nparam
    RSS <- sum(res^2)
    sigma2 <- RSS / n
    # likelihood of n NID observations
    AIC <- n * log(RSS) + 2 * (k + 1)
    BIC <- n * log(RSS) + (k + 1) * log(n)
    
    result <- list(x=x, x.hat=x.hat, res=res, RSS=RSS, sigma2=sigma2, AIC=AIC, BIC=BIC)
    return(result)
}

alpha0 <- 0.1
alpha1 <- 0.8

es_0 <- simpleExpSmooth(x, alpha0)
es_1 <- simpleExpSmooth(x, alpha1)

xp0 <- es_0$x.hat
xp1 <- es_1$x.hat

# 1-step predictions
x.hat0 <- c(xp0[nt], numeric(ne))
x.hat1 <- c(xp1[nt], numeric(ne))

for(t in 2:(ne + 1)) {
  x.hat0[t] <- as.numeric(alpha0 * X[nt + t - 1] + (1 - alpha0) * x.hat0[t - 1])
  x.hat1[t] <- as.numeric(alpha1 * X[nt + t - 1] + (1 - alpha1) * x.hat1[t - 1])
}

```

The errors $\epsilon_t^{Add} = x - \hat{x}_{t|t-1}$ and $\epsilon_t^{Mult} = \frac{x - \hat{x}_{t|t-1}}{\hat{x}_{t|t-1}}$ can be reduced to the formula we used for pointwise smoothing: $\hat{x}_{t|t} = \alpha x_t + (1 - \alpha) \hat{x}_{t|t-1}$.
The model $AIC$ and $BIC$ are computed with respect to the total of two estimated parameters: initial state and $\alpha$.

```{r simpleEtsEstim, fig.width=12, fig.height=5}
sublab <- paste("RMSE = ", round(sqrt( sum( (x.hat0 - X[nt:length(X)])^2) / (ne - 1)), 4), ", ", round(sqrt( sum( (x.hat1 - X[nt:length(X)])^2) / (ne - 1)), 4), sep="")
plot(x=1:length(X), y=X, main="Simple exponential smoothing, ETS(A, N, N)", sub=sublab, xlab="day", ylab="[°C]")
mtext(paste(" BIC = ", round(es_0$BIC, 4), ", ", round(es_1$BIC, 4), sep=""))
lines(xp0, lwd=2, col="brown")
lines(xp1, lwd=1, lty=20, col="brown3")
lines(x=nt:length(X), y=x.hat0, lwd=2, col="blue")
lines(x=nt:length(X), y=x.hat1, lwd=1, lty=20, col="blue3")
legend("topleft", legend=c(paste("ETS(A, N, N), alpha = ", alpha0), paste("1-step prediction, alpha=", alpha0),
                           paste("ETS(A, N, N), alpha = ", alpha1), paste("1-step prediction, alpha=", alpha1),"data (residuals)"),
         col=c("brown", "blue", "brown", "blue3", "black"), lty=c(1, 1, 20, 20, NA), lwd=c(2, 2, 1, 1, 1), cex=0.8, pch=c(NA, NA, NA, NA, 1))
```

We observe that with $\alpha \rightarrow 1^-$ the simple ETS model fits the data more precisely, but at the same time does not account for the error terms properly. 
  Models with different smoothing parameters need to be examined according to some information criterion. 

```{r} 

models.ets <- list()
for (alpha in seq(from = 0.001, to = 0.999, by = 0.001)) {
  ses_estim <- simpleExpSmooth(x, alpha)
  models.ets[[paste("SES(alpha = ",alpha,")")]] <- c(alpha=alpha, sigma2=ses_estim$sigma2, aic=ses_estim$AIC, bic=ses_estim$BIC)
}

models.ets <- data.frame(matrix(unlist(models.ets), nrow=length(models.ets), byrow=T))
names(models.ets) <- c("alpha", "sigma^2", "AIC", "BIC")
head(models.ets)

```

Now that we searched through 1000 simple ES models with distinct smoothing parameters $\alpha$ we choose the first 2 with the lowest $BIC$:

```{r etsBicPlot, fig.width=7, fig.height=5}

bic <- models.ets[,4]
bestEts <- models.ets[order(bic),]
head(bestEts)

plot(x=seq(from = 0.001, to = 0.999, by = 0.001), y=models.ets[,4], xlab="alpha", ylab="BIC", main="BIC of simple ES models", pch=20, type="b", col="blue4", ylim=c(1700, 2500))
mtext("\\w respect to alpha")
lines(x=c(0, 0.95), y=c(bestEts[1,4], bestEts[1,4]), col="red", lty=2, lwd=2)
lines(x=c(bestEts[1,1], bestEts[1,1]), y=c(0, bestEts[1,4]), col="red", lty=2, lwd=2)


```

### 7.2 Automatic ETS Model Search Method

Using an `ets` method from the `forecast` package we obtain the same best model:

```{r etsEstim, fig.width=10, fig.height=5}
require(forecast)
( fc_ets <- ets(x) )
plot(fc_ets)

```

As we notice, the value of smoothing parameter $\alpha$ closer to 1 has a much more subtle smoothing effect than that of models with smaller $\alpha$s. The `ets` method examines a much larger search space than the one we chose for demonstration. In fact, it tests for multiple available combinations of ETS models with additional parameters: $\beta$ (trend), $\gamma$ (season), and $\phi$ (trend damping).

  The method also allows multiple-step predictions which we can then display with seasonal fit included:

```{r etsPredict, fig.width=10, fig.height=5}
fc_ets.fc <- predict(fc_ets, h=ne, level=c(80, 95))
plot(fc_ets.fc)
lines(x=1:(nt + 1), y=fc_ets$states[,1], lwd=2, lty=2, col="red")
```

### 7.3 Single and Multi-Step Prediction Estimates

Each state variable is a 1-step prediction of the previous values. This makes 1-step predictions relatively straight-forward, as observed on eval parts of chosen models in the beginning of section 7.1. To predict $h > 1$ steps ahead we make use of the state space equations to accumulate the levels as well as additional ES parameters. <br/>
  For an additive damped ETS model with trend we use:

  $a_t = \alpha  x_t + (1 - \alpha) (a_{t - 1} + \phi b_{t - 1})$ <br/>
  $b_t = \beta (a_t - a_{t - 1}) + (1 - \beta) \phi  b_{t - 1}$ <br/>
  $\hat{x}_{t|t-1} = a_{t - 1} + \phi b_{t - 1}$ <br/>
  
  and  <br/>
  
  $a_t = \alpha  x_t + (1 - \alpha) (a_{t - 1} b_{t - 1}^{\phi})$ <br/>
  $b_t = \beta (a_t / a_{t - 1}) + (1 - \beta) b_{t - 1}^{\phi}$ <br/>
  $\hat{x}_{t|t-1} = a_{t - 1} b_{t - 1}^{\phi}$ <br/>
  
  for the multiplicative trend. Both with a damping coefficient $\phi$.

The $h$-step predictions will then be carried out as followed: <br/>

  $\hat{x}_{t + h|t} = a_{t} + (\phi + \phi^2 + ... + \phi^h) b_{t}$ <br/>

for an additive trend, and: <br/>

  $\hat{x}_{t + h|t} = a_{t} + b_{t}^{(\phi + \phi^2 + ... + \phi^h)}$ <br/>
  
for a multiplicative trend.

```{r}

ExpSmoothWithTrend <- function(x, alpha, beta = 0.1, phi = 1, h = 0, multiplicative = F) {
  n <- length(x)
  
  a <- c(x[1], numeric(n - 1))
  b <- numeric(n)
  b[1] <- ifelse(multiplicative, x[2] / x[1], (x[n] - x[1]) / n)
  x.hat <- c(x[1], numeric(n - 1))

  for (i in 2:n) {
    if (multiplicative) {
      a[i] <- alpha * x[i] + (1 - alpha) * (a[i - 1] * b[i - 1] ^ phi)
      b[i] <- beta * (a[i] / a[i - 1]) + (1 - beta) * b[i - 1] ^ phi
      x.hat[i] <- a[i - 1] * b[i - 1] ^ phi
    } else {
      a[i] <- alpha * x[i] + (1 - alpha) * (a[i - 1] + phi * b[i - 1])
      b[i] <- beta * (a[i] - a[i - 1]) + (1 - beta) * phi * b[i - 1]
      x.hat[i] <- a[i - 1] + phi * b[i - 1]
    }
  }
  
  xp <- numeric(h)
  if (h > 0) {
    if (multiplicative) {
      xp <- a[n] + b[n]^(cumsum(phi^(1:h)))
    } else {
      xp <- a[n] + cumsum(phi^(1:h)) * b[n]
    }
  }
    
  res <- (x - x.hat)
  k <- 3 # nparam
  RSS <- sum(res^2)
  sigma2 <- RSS / n
  # likelihood of n NID observations
  AIC <- n * log(RSS) + 2 * (k + 1)
  BIC <- n * log(RSS) + (k + 1) * log(n)
  
  result <- list(x=x, x.hat=x.hat, x.pred=xp, res=res, RSS=RSS, sigma2=sigma2, AIC=AIC, BIC=BIC)
  return(result)
}

```

This method was proposed by Gardner and McKenzie in 1985 as an extension of Holt's method of ES with trend. We will use trend damping ($\phi \in [0, 1]$) to estimate long-term predictions:

```{r esWithDampedTrend, fig.width=10, fig.height=5}

alpha <- bestEts[1,1]
h <- ne

ets_AAN <- ExpSmoothWithTrend(x, alpha, beta=0.1, phi=0.98, h)
ets_AMN <- ExpSmoothWithTrend(x, alpha, beta=0.001, phi=0.99, h, multiplicative=T)

x.pred_additive <- ets_AAN$x.pred
x.pred_multiplicative <- ets_AMN$x.pred

plot(x, type="l", xlim=c(nt - h, nt + h), main=paste(h,"-step predictions", sep=""), xlab="day", ylab="[°C]")
mtext(paste("alpha = ",alpha ,sep=""))
lines(x=nt:(nt + 1), y=c(x[nt], x.pred_additive[1]), col="red2", lwd=2, lty=2)
lines(x=(nt + 1):(nt + h), y=x.pred_additive, col="red2", lwd=2)
lines(x=nt:(nt + 1), y=c(x[nt], x.pred_multiplicative[1]), col="blue2", lwd=2, lty=2)
lines(x=(nt + 1):(nt + h), y=x.pred_multiplicative, col="blue2", lwd=2)
legend("topright", legend=c(paste("ETS(A,A,N), beta = ", 0.1, ", phi = ", 0.98, sep=""), paste("ETS(A,M,N), beta = ", 0.001, ", phi = ", 0.99, sep="")), col=c("red2", "blue2"), lty=c(1, 1), lwd=c(2, 2), cex=0.8)
```

The multiplicative version of this method seems to work for $\beta \ll \phi$, as the cumulative sum in the exponent causes the predictions to diverge for trend values $b_n > 1$.

 When $\beta = 0$ the ETS model becomes a simple level smoothing model, leading to a constant long-term prediction. Increasing $\beta$ to values slightly above zero leads to a prediction with an overdamped trend. The trend becomes more apparent as we increase $\beta$ up to a point where it is no longer overdamped by $\phi$, as we can see in the plot below:

```{r changingBetaParam, fig.width=10, fig.height=5}
suppressMessages(pkgTest("aTSA"))
optim_beta <- 0.01; optim_RMSE <- Inf
plot(x, type="l", xlim=c(nt - h, nt + h), main=paste(h,"-step predictions \\w increasing trend params", sep=""), xlab="day", ylab="[°C]", ylim=c(4.5, 20))
mtext(paste("alpha = ",alpha, ", beta: 0.01 to 0.4 \\w step: 0.1" ,sep=""))
for (beta in seq(from=0.01, to=0.4, by=0.01)) {
  h2p <- Holt(x, alpha=alpha, beta=beta, lead = h, damped=T, plot=F)$pred
  rmse <- sqrt(mean((temp_eval$T - h2p)^2))
  if ( rmse < optim_RMSE) {
    optim_RMSE <- rmse
    optim_beta <- beta
  }
  lines(x=nt:(nt + 1), y=c(x[nt], h2p[1]), col=hsv((32 - 28 * beta) / 40, v=0.8), lwd=2, lty=2)
  lines(x=(nt + 1):(nt + h), y=h2p, col=hsv((32 - 28 * beta) / 40, v=0.8), lwd=2)
}
legend("bottomleft", legend=c("beta = 0.01", "beta = 0.4"), col=c(hsv((32 - 28 * 0.01) / 40, v=0.8), hsv((32 - 28 * 0.4) / 40, v=0.8)), lwd=2, cex=0.8)

```

Naturally, we want to choose $\beta$ such that the damped trend predictions describe the evaluation part most accurately.

```{r longTermBetaPred, fig.width=10, fig.height=5}
holt_damped <- Holt(x, alpha=alpha, beta=optim_beta, lead = h, damped=T, plot=F)
h2pred <- holt_damped$pred
plot(x, type="p", xlim=c(nt - h, nt + h), main=paste("Damped 2-parametric ",h,"-step prediction", sep=""), xlab="day", ylab="[°C]")
mtext(paste("alpha = ",alpha, ", beta = ", optim_beta ,sep=""))
lines(x=1:nt, y=simpleExpSmooth(x, alpha)$x.hat, lwd=2, lty=2, col="red2")
lines(x=1:nt, y=holt_damped$estimate, lwd=2, col="green3")
lines(x=(nt + 1):(nt + ne), y=temp_eval$T, lwd=2, col="red", type="p", pch=20)
lines(x=nt:(nt + 1), y=c(x[nt], h2pred[1]), col="blue2", lwd=2, lty=2)
lines(x=(nt + 1):(nt + h), y=h2pred, col="blue2", lwd=2)
legend("bottomleft", legend=c(paste("prediction (", ne,"-step)", sep=""), "simple es", "damped Holt", "data"),
       col=c("blue2", "red2", "green3","red"), lty=c(1, 2, 1, NA), lwd=2 , cex=0.8, pch=c(NA, NA, NA, 20))
```

### 7.4 ETS Model Comparison (Conclusion)

Since the data contains no apparent or significant trend, nor seasonality, we only examined simple ETS models. The first two results of a $BIC$ search are:

```{r}
bestEts[1:2,]
```

The $\alpha$ parameter values do not differ much from one another and they correspond to the search from `forecast`'s `ets` function, hence we settle down for $\alpha = 0.747$.
However if we include damped trend with $\beta = 0.25$ and damping coefficient $\phi = 0.98$ (included in `Holt` method from `aTSA` package) we get a closer fit with $\sigma_{\varepsilon}^2$:

```{r}
RSS <- sum((holt_damped$estimate - x)^2)
RSS / (nt - 3)
```
and $BIC$:

```{r}
nt * log(sum((holt_damped$estimate - x)^2)) + 4 * log(nt)
```

We can compare the predictive abilities of models $ETS(A,N,N)$ and $ETS(A,A_d,N)$ with the DM test:

```{r}
ses_whole <- simpleExpSmooth(X, alpha=alpha)
holt_damped_whole <- Holt(X, alpha=alpha, beta=optim_beta, damped=T, plot=F)

e_1 <- ses_whole$x.hat[(nt + 1):length(X)] - X[(nt + 1):length(X)]
e_2 <- holt_damped_whole$estimate[(nt + 1):length(X)] - X[(nt + 1):length(X)]

dm.test(e_1, e_2)
```

which suggests significantly better predictive properties of the simple $ETS(A,N,N)$ with $\alpha = 0.747$. The RMSE's of the above models are:

```{r}
( rmse_1 <- sqrt(mean(e_1)) )
```
and 
```{r}
( rmse_2 <- sqrt(mean(e_2)) )
```

## Discussion

We have evaluated daily averaged temperature measurements taken over year 2015 on a wind farm with the basic approaches of time series analysis, namely: decomposition, modeling residuals as a stationary (ARMA) process and introduced integrated processes and exponential smoothing (ETS) as well.

The data contains a clear seasonal component with a 365-day period, but due to the length of the sample being shorter than the period itself we were unable to model the data using seasonal integrated models which require the data to contain at least two periods. We were left with the following approaches:

(a) Decomposition and ARMA for residuals: $\hat{x}_t = \hat{x}_t^{Seas} + \hat{x}_t^{ARMA} + \varepsilon_t$ <br/>

(b) Modeling with ARIMA and ARFIMA models: $\hat{x}_t = \hat{x}_t^I + \varepsilon_t $ where $I \in \{ARIMA, ARFIMA\}$<br/>

(c) Using exponential smoothing: $\hat{x}_t = \hat{x}_{t}^{ETS} + \varepsilon_t$ <br/>

Technique (a) yields the best results since it clearly addresses the seasonality of the data and then models the residuals as stationary ARMA processes:

```{r}
head(lowestBIC)
```

Yet compared with ETS models in (c), more precisely $ETS(A,N,N),\alpha=0.747$ with $RMSE(1) = 0.220368$ they do not posses significantly better predictive abilites than the best ARMA model, as shown through the DM-test:

```{r}
e_arma <- (temp_eval$Tres - eval.predictions[[1]])
dm.test(e_1, e_arma)
```

Approach (b) was only used as an exercise with no significant difference in predictive abilities (merely between two specific ARIMA models).
The residuals $\varepsilon_t$ in approach (a) can be considered random even though Diagnostic tests such as the Jarque-Bera test rejected their normality. The same can be applied on the residuals of approach (c):

```{r, echo=T}
tseries::jarque.bera.test(e_1) # normality of ETS resids

randtests::runs.test(e_1) # randomness of ETS resids

randtests::runs.test(e_arma) # randomness of ARMA resids
```
